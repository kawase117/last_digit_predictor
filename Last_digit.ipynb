{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# セル1: 環境セットアップ（1回実行）\n",
    "# ============================================================\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 設定\n",
    "DB_PATH = 'pachinko_analysis_マルハンメガシティ柏.db'\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# イベント定義（グローバル）\n",
    "EVENT_DEFINITIONS = {\n",
    "    'is_1day': '1day', 'is_2day': '2day', 'is_3day': '3day',\n",
    "    'is_4day': '4day', 'is_5day': '5day', 'is_6day': '6day',\n",
    "    'is_7day': '7day', 'is_8day': '8day', 'is_9day': '9day',\n",
    "    'is_0day': '0day', 'is_39day': '39day', 'is_40day': '40day',\n",
    "    'is_zorome': 'Zorome', 'is_saturday': 'Saturday', 'is_sunday': 'Sunday'\n",
    "}\n",
    "\n",
    "DIGIT_ORDER = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'ゾロ目']\n",
    "\n",
    "print(\"✅ 環境セットアップ完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# セル2: データ読み込み（1回実行）\n",
    "# ============================================================\n",
    "\n",
    "def load_and_prepare_data(db_path):\n",
    "    \"\"\"全データ読込・整形\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # 3テーブル読込\n",
    "    tables = {\n",
    "        'all': 'last_digit_summary_all',\n",
    "        'jug': 'last_digit_summary_jug',\n",
    "        'non_jug': 'last_digit_summary_other'\n",
    "    }\n",
    "    \n",
    "    df_events = pd.read_sql_query(\"SELECT * FROM event_calendar ORDER BY date\", conn)\n",
    "    data = {}\n",
    "    \n",
    "    for key, table in tables.items():\n",
    "        try:\n",
    "            df = pd.read_sql_query(f\"SELECT * FROM {table} ORDER BY date, last_digit\", conn)\n",
    "            df = df.merge(df_events, on='date', how='left')\n",
    "            \n",
    "            # 日付処理\n",
    "            df['date_obj'] = pd.to_datetime(df['date'], format='%Y%m%d')\n",
    "            df['weekday'] = df['date_obj'].dt.day_name()\n",
    "            df['weekday_num'] = df['date_obj'].dt.weekday\n",
    "            df['is_saturday'] = (df['weekday_num'] == 5).astype(int)\n",
    "            df['is_sunday'] = (df['weekday_num'] == 6).astype(int)\n",
    "            df['last_digit'] = pd.Categorical(df['last_digit'], categories=DIGIT_ORDER, ordered=True)\n",
    "            \n",
    "            data[key] = df\n",
    "            print(f\"{key}: {len(df)} rows, {df['date'].min()} - {df['date'].max()} ({df['date'].nunique()} days)\")\n",
    "        except:\n",
    "            data[key] = pd.DataFrame()\n",
    "    \n",
    "    conn.close()\n",
    "    return data['all'], data['jug'], data['non_jug']\n",
    "\n",
    "df_all, df_jug, df_non_jug = load_and_prepare_data(DB_PATH)\n",
    "\n",
    "print(\"\\n✅ データ読み込み完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# セル3: 共通関数定義（1回実行）\n",
    "# ============================================================\n",
    "\n",
    "def create_event_history_features(df):\n",
    "    \"\"\"イベント履歴特徴量作成（インデックス化版）\"\"\"\n",
    "    all_dates = sorted(df['date'].unique())\n",
    "    \n",
    "    # インデックス構築: {(event_type, digit): [date1, date2, ...]}\n",
    "    print(\"  インデックス構築中...\")\n",
    "    event_digit_index = {}\n",
    "    \n",
    "    for date in all_dates:\n",
    "        date_mask = df['date'] == date\n",
    "        date_data = df[date_mask].iloc[0]\n",
    "        \n",
    "        # アクティブイベント特定\n",
    "        active_events = []\n",
    "        for col, label in EVENT_DEFINITIONS.items():\n",
    "            if col in df.columns:  # ← DataFrameのカラムで確認\n",
    "                if date_data.get(col, 0) == 1:\n",
    "                    active_events.append(label.lower())\n",
    "        \n",
    "        # 各末尾×イベントの組み合わせにdateを追加\n",
    "        for digit in df[date_mask]['last_digit'].unique():\n",
    "            for event_name in active_events:\n",
    "                key = (event_name, digit)\n",
    "                if key not in event_digit_index:\n",
    "                    event_digit_index[key] = []\n",
    "                event_digit_index[key].append(date)\n",
    "    \n",
    "    # 差枚キャッシュ: {(date, digit): diff}\n",
    "    diff_cache = df.set_index(['date', 'last_digit'])['avg_diff_coins'].to_dict()\n",
    "    \n",
    "    # 特徴量生成\n",
    "    print(\"  特徴量生成中...\")\n",
    "    history_features = []\n",
    "    \n",
    "    for (event_name, digit), dates in event_digit_index.items():\n",
    "        for i, date in enumerate(dates):\n",
    "            current_diff = diff_cache.get((date, digit))\n",
    "            if current_diff is None:\n",
    "                continue\n",
    "            \n",
    "            # 過去の差枚を直接取得\n",
    "            prev_1 = diff_cache.get((dates[i-1], digit)) if i >= 1 else None\n",
    "            prev_2 = diff_cache.get((dates[i-2], digit)) if i >= 2 else None\n",
    "            \n",
    "            history_features.append({\n",
    "                'date': date, 'last_digit': digit, 'event_type': event_name,\n",
    "                'current_diff': current_diff, 'prev_1_diff': prev_1, 'prev_2_diff': prev_2,\n",
    "                'prev_1_change': current_diff - prev_1 if prev_1 else None,\n",
    "                'prev_2_change': current_diff - prev_2 if prev_2 else None,\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(history_features)\n",
    "\n",
    "def prepare_enhanced_features(history_df, event_type, cutoff_date, include_current=False):\n",
    "    \"\"\"機械学習用特徴量準備\n",
    "    \n",
    "    Args:\n",
    "        include_current: Trueの場合cutoff_dateを含む（学習時）、Falseで除外（予測時）\n",
    "    \"\"\"\n",
    "    if include_current:\n",
    "        event_data = history_df[\n",
    "            (history_df['event_type'] == event_type) & \n",
    "            (history_df['date'] <= cutoff_date)\n",
    "        ].copy()\n",
    "    else:\n",
    "        event_data = history_df[\n",
    "            (history_df['event_type'] == event_type) & \n",
    "            (history_df['date'] < cutoff_date)\n",
    "        ].copy()\n",
    "    \n",
    "    if len(event_data) == 0:\n",
    "        return None, None, None\n",
    "    \n",
    "    X = event_data[['prev_1_diff', 'prev_2_diff', 'prev_1_change', 'prev_2_change']].fillna(0)\n",
    "    y = (event_data['current_diff'] > event_data['current_diff'].median()).astype(int)\n",
    "    \n",
    "    return X, y, event_data\n",
    "\n",
    "def analyze_trend(df, recent_days=30):\n",
    "    \"\"\"全期間 vs 直近トレンド分析\"\"\"\n",
    "    latest_date = sorted(df['date'].unique())[-1]\n",
    "    cutoff_date = (pd.to_datetime(latest_date, format='%Y%m%d') - timedelta(days=recent_days)).strftime('%Y%m%d')\n",
    "    \n",
    "    trends = {}\n",
    "    for col, label in EVENT_DEFINITIONS.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        event_data = df[df[col] == 1] if col.startswith('is_') else df[df['weekday_num'] == (5 if 'saturday' in col else 6)]\n",
    "        recent_data = event_data[event_data['date'] >= cutoff_date]\n",
    "        \n",
    "        trends[label.lower()] = {}\n",
    "        for digit in df['last_digit'].unique():\n",
    "            all_avg = event_data[event_data['last_digit'] == digit]['avg_diff_coins'].mean()\n",
    "            recent_avg = recent_data[recent_data['last_digit'] == digit]['avg_diff_coins'].mean()\n",
    "            \n",
    "            if pd.notna(all_avg) and pd.notna(recent_avg):\n",
    "                trends[label.lower()][str(digit)] = {\n",
    "                    'all_period_avg': round(all_avg, 1),\n",
    "                    'recent_avg': round(recent_avg, 1),\n",
    "                    'trend': round(recent_avg - all_avg, 1)\n",
    "                }\n",
    "    \n",
    "    return trends\n",
    "\n",
    "print(\"✅ 共通関数定義完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# セル4: イベント履歴・トレンド分析実行（1回実行）\n",
    "# ============================================================\n",
    "\n",
    "# 3モデル分の履歴作成（df_all, df_jug, df_non_jugを直接使用）\n",
    "histories = {}\n",
    "trends = {}\n",
    "\n",
    "for name, df in [('all', df_all), ('jug', df_jug), ('non_jug', df_non_jug)]:\n",
    "    if len(df) > 0:\n",
    "        print(f\"【{name.upper()}】履歴作成中...\")\n",
    "        histories[name] = create_event_history_features(df)\n",
    "        print(f\"  {len(histories[name])} records, 欠損率: prev_1={histories[name]['prev_1_diff'].isna().mean():.1%}\")\n",
    "        \n",
    "        trends[name] = analyze_trend(df, recent_days=30)\n",
    "\n",
    "print(\"\\n✅ 履歴・トレンド分析完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TOP1/TOP2取得回数ヒートマップ\n",
    "# ============================================================\n",
    "\n",
    "def plot_top_rank_heatmap(df, title, rank_type='top1'):\n",
    "    \"\"\"\n",
    "    TOP1またはTOP2を取った回数のヒートマップ\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        分析対象データ\n",
    "    title : str\n",
    "        タイトル\n",
    "    rank_type : str\n",
    "        'top1' or 'top2'\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # イベント×末尾でTOPランク取得回数を集計\n",
    "    pivot_data = []\n",
    "    \n",
    "    for col, label in EVENT_DEFINITIONS.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        \n",
    "        event_data = df[df[col] == 1].copy()\n",
    "        \n",
    "        if len(event_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # 各日のランク計算\n",
    "        top_counts = {}\n",
    "        for digit in DIGIT_ORDER:\n",
    "            top_counts[digit] = 0\n",
    "        \n",
    "        for date in event_data['date'].unique():\n",
    "            date_data = event_data[event_data['date'] == date].copy()\n",
    "            date_data = date_data.sort_values('avg_diff_coins', ascending=False)\n",
    "            \n",
    "            if rank_type == 'top1' and len(date_data) >= 1:\n",
    "                top_digit = date_data.iloc[0]['last_digit']\n",
    "                top_counts[top_digit] += 1\n",
    "            elif rank_type == 'top2' and len(date_data) >= 2:\n",
    "                top2_digit = date_data.iloc[1]['last_digit']\n",
    "                top_counts[top2_digit] += 1\n",
    "        \n",
    "        pivot_data.append(pd.Series(top_counts, name=label))\n",
    "    \n",
    "    if len(pivot_data) == 0:\n",
    "        print(f\"No data for {title}\")\n",
    "        return\n",
    "    \n",
    "    pivot_df = pd.concat(pivot_data, axis=1).T\n",
    "    \n",
    "    # ヒートマップ描画\n",
    "    fig, ax = plt.subplots(figsize=(14, max(8, len(pivot_data) * 0.5)))\n",
    "    \n",
    "    rank_label = 'TOP1' if rank_type == 'top1' else 'TOP2'\n",
    "    cmap = 'YlOrRd' if rank_type == 'top1' else 'YlGnBu'\n",
    "    \n",
    "    sns.heatmap(\n",
    "        pivot_df, \n",
    "        annot=True, \n",
    "        fmt='.0f', \n",
    "        cmap=cmap, \n",
    "        ax=ax,\n",
    "        cbar_kws={'label': f'{rank_label} Count'}\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f'{title} - {rank_label} Count by Digit x Event', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Last Digit', fontsize=12)\n",
    "    ax.set_ylabel('Event Type', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 実行\n",
    "print(\"=\"*100)\n",
    "print(\"【TOP1/TOP2取得回数ヒートマップ】\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# データ読み込み(未読み込みの場合)\n",
    "if 'df_all' not in globals():\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    df_all = pd.read_sql_query(\"SELECT * FROM last_digit_summary_all\", conn)\n",
    "    df_events = pd.read_sql_query(\"SELECT * FROM event_calendar\", conn)\n",
    "    df_all = df_all.merge(df_events, on='date', how='left')\n",
    "    df_all['date_obj'] = pd.to_datetime(df_all['date'], format='%Y%m%d')\n",
    "    df_all['weekday_num'] = df_all['date_obj'].dt.weekday\n",
    "    df_all['last_digit'] = pd.Categorical(df_all['last_digit'], \n",
    "                                          categories=DIGIT_ORDER, \n",
    "                                          ordered=True)\n",
    "    conn.close()\n",
    "\n",
    "# TOP1ヒートマップ\n",
    "print(\"\\n【TOP1取得回数】\")\n",
    "plot_top_rank_heatmap(df_all, 'ALL', rank_type='top1')\n",
    "\n",
    "# TOP2ヒートマップ\n",
    "print(\"\\n【TOP2取得回数】\")\n",
    "plot_top_rank_heatmap(df_all, 'ALL', rank_type='top2')\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"✅ ヒートマップ表示完了\")\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# セル5-1: ヒートマップ表示（1回実行）\n",
    "# ============================================================\n",
    "\n",
    "def plot_heatmap(df, title):\n",
    "    \"\"\"ヒートマップのみ表示\"\"\"\n",
    "    df = df.copy()\n",
    "    df['is_saturday'] = (df['weekday_num'] == 5).astype(int)\n",
    "    df['is_sunday'] = (df['weekday_num'] == 6).astype(int)\n",
    "    \n",
    "    # ヒートマップ用データ\n",
    "    pivot_data = []\n",
    "    for col, label in EVENT_DEFINITIONS.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        event_data = df[df[col] == 1]\n",
    "        if len(event_data) > 0:\n",
    "            pivot_data.append(event_data.groupby('last_digit')['avg_diff_coins'].mean().rename(label))\n",
    "    \n",
    "    if len(pivot_data) == 0:\n",
    "        print(f\"No data for {title}\")\n",
    "        return\n",
    "    \n",
    "    pivot_df = pd.concat(pivot_data, axis=1).T\n",
    "    \n",
    "    # ヒートマップ描画\n",
    "    fig, ax = plt.subplots(figsize=(14, max(8, len(pivot_data) * 0.5)))\n",
    "    sns.heatmap(pivot_df, annot=True, fmt='.0f', cmap='RdYlGn', center=0, ax=ax, \n",
    "                cbar_kws={'label': 'Avg Diff Coins'})\n",
    "    ax.set_title(f'{title} - Avg Diff by Digit x Event', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Last Digit', fontsize=12)\n",
    "    ax.set_ylabel('Event Type', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 全モデル実行\n",
    "data_dict = {'all': df_all, 'jug': df_jug, 'non_jug': df_non_jug}\n",
    "\n",
    "for name, df in data_dict.items():\n",
    "    if len(df) > 0:\n",
    "        print(f\"【{name.upper()}】\")\n",
    "        plot_heatmap(df, name.upper())\n",
    "        print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "print(\"✅ ヒートマップ表示完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# セル5-2: LLM用JSONデータ出力（1回実行）\n",
    "# ============================================================\n",
    "\n",
    "def export_json_data(trend_data, title):\n",
    "    \"\"\"JSONデータ出力\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"【{title} - LLM用データ】\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(json.dumps(trend_data, indent=2, ensure_ascii=False))\n",
    "\n",
    "# 全モデル実行\n",
    "print(\"=\"*100)\n",
    "print(\"AI分析用テキストデータ出力開始\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for name in ['all', 'jug', 'non_jug']:\n",
    "    if name in trends and len(trends[name]) > 0:\n",
    "        export_json_data(trends[name], name.upper())\n",
    "        print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "\n",
    "print(\"✅ LLM用データ出力完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# セル6: LogReg学習（1回実行）\n",
    "# ============================================================\n",
    "\n",
    "def train_logreg_all_events(history_df, model_name, n_test=5):\n",
    "    \"\"\"全イベントタイプ一括学習\"\"\"\n",
    "    results_all = {}\n",
    "    models_all = {}\n",
    "    \n",
    "    for event_type in sorted(history_df['event_type'].unique()):\n",
    "        event_history = history_df[history_df['event_type'] == event_type].copy()\n",
    "        event_dates = sorted(event_history['date'].unique())\n",
    "        \n",
    "        if len(event_dates) < n_test + 3:\n",
    "            continue\n",
    "        \n",
    "        test_dates = event_dates[-n_test:]\n",
    "        results = []\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"【{event_type.upper()} - {model_name}】\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        for test_date in test_dates:\n",
    "            train_cutoff = event_dates[event_dates.index(test_date) - 1]\n",
    "            X_train, y_train, _ = prepare_enhanced_features(history_df, event_type, train_cutoff, include_current=True)\n",
    "            \n",
    "            if X_train is None or len(X_train) == 0:\n",
    "                continue\n",
    "            \n",
    "            # テストデータ: test_dateを除外して特徴量のみ取得\n",
    "            test_data = event_history[event_history['date'] == test_date]\n",
    "            X_test_base, _, _ = prepare_enhanced_features(history_df, event_type, test_date, include_current=False)\n",
    "            \n",
    "            if X_test_base is None:\n",
    "                continue\n",
    "            \n",
    "            # test_date当日の末尾に対応する特徴量を取得\n",
    "            X_test_list = []\n",
    "            for digit in test_data['last_digit'].values:\n",
    "                # この末尾の最新特徴量（test_date以前）\n",
    "                digit_history = history_df[\n",
    "                    (history_df['event_type'] == event_type) & \n",
    "                    (history_df['last_digit'] == digit) &\n",
    "                    (history_df['date'] < test_date)\n",
    "                ]\n",
    "                if len(digit_history) > 0:\n",
    "                    latest = digit_history.iloc[-1]\n",
    "                    X_test_list.append([\n",
    "                        latest['prev_1_diff'], latest['prev_2_diff'],\n",
    "                        latest['prev_1_change'], latest['prev_2_change']\n",
    "                    ])\n",
    "                else:\n",
    "                    X_test_list.append([0, 0, 0, 0])\n",
    "            \n",
    "            X_test = pd.DataFrame(X_test_list, columns=['prev_1_diff', 'prev_2_diff', 'prev_1_change', 'prev_2_change'])\n",
    "            \n",
    "            # 学習・予測\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train.fillna(0))\n",
    "            X_test_scaled = scaler.transform(X_test.fillna(0))\n",
    "            \n",
    "            model = LogisticRegression(C=1.0, class_weight='balanced', max_iter=1000, random_state=42)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "            predicted_top3 = [d for d, _ in sorted(zip(test_data['last_digit'].values, proba), \n",
    "                                                    key=lambda x: x[1], reverse=True)[:3]]\n",
    "            actual_top3 = test_data.nlargest(3, 'current_diff')['last_digit'].tolist()\n",
    "            overlap = len(set(predicted_top3) & set(actual_top3))\n",
    "            \n",
    "            results.append({\n",
    "                'date': test_date, 'predicted_top3': predicted_top3,\n",
    "                'actual_top3': actual_top3, 'overlap': overlap, 'precision': overlap / 3\n",
    "            })\n",
    "        \n",
    "        if len(results) > 0:\n",
    "            df_results = pd.DataFrame(results)\n",
    "            print(f\"{'日付':<12} {'予測TOP3':<30} {'実際TOP3':<30} {'的中':<6} {'Precision':<10}\")\n",
    "            print(\"-\" * 100)\n",
    "            for _, row in df_results.iterrows():\n",
    "                pred = ','.join([str(d) for d in row['predicted_top3']])\n",
    "                actual = ','.join([str(d) for d in row['actual_top3']])\n",
    "                print(f\"{row['date']:<12} {pred:<30} {actual:<30} {row['overlap']:<6} {row['precision']:<10.3f}\")\n",
    "            print(f\"\\n平均 Precision@3: {df_results['precision'].mean():.3f}\")\n",
    "            \n",
    "            results_all[event_type] = df_results\n",
    "            \n",
    "            # 最終モデル学習\n",
    "            X_all, y_all, _ = prepare_enhanced_features(history_df, event_type, event_dates[-1])\n",
    "            if X_all is not None:\n",
    "                scaler_final = StandardScaler()\n",
    "                X_all_scaled = scaler_final.fit_transform(X_all.fillna(0))\n",
    "                model_final = LogisticRegression(C=1.0, class_weight='balanced', max_iter=1000, random_state=42)\n",
    "                model_final.fit(X_all_scaled, y_all)\n",
    "                models_all[event_type] = {'model': model_final, 'scaler': scaler_final}\n",
    "    \n",
    "    return results_all, models_all\n",
    "\n",
    "# 3モデル学習\n",
    "all_logreg_results = {}\n",
    "all_logreg_models = {}\n",
    "\n",
    "for name in ['all', 'jug', 'non_jug']:\n",
    "    if name in histories and len(histories[name]) > 0:\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"{name.upper()} LogReg学習\")\n",
    "        print(f\"{'='*100}\")\n",
    "        results, models = train_logreg_all_events(histories[name], name.upper(), n_test=5)\n",
    "        all_logreg_results[name] = results\n",
    "        all_logreg_models[name] = models\n",
    "\n",
    "print(\"\\n✅ LogReg学習完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# セル7: 次回イベント予測（1回実行）\n",
    "# ============================================================\n",
    "\n",
    "def predict_next_event(history_df, event_type, model_info):\n",
    "    \"\"\"次回イベント予測\"\"\"\n",
    "    event_history = history_df[history_df['event_type'] == event_type].copy()\n",
    "    if len(event_history) == 0:\n",
    "        return None\n",
    "    \n",
    "    model, scaler = model_info['model'], model_info['scaler']\n",
    "    latest_date = event_history['date'].max()\n",
    "    \n",
    "    X_all, _, _ = prepare_enhanced_features(history_df, event_type, latest_date)\n",
    "    if X_all is None:\n",
    "        return None\n",
    "    \n",
    "    latest_data = event_history[event_history['date'] == latest_date]\n",
    "    X_latest = X_all.iloc[-len(latest_data):].fillna(0)\n",
    "    \n",
    "    proba = model.predict_proba(scaler.transform(X_latest))[:, 1]\n",
    "    return pd.DataFrame({\n",
    "        'digit': latest_data['last_digit'].values,\n",
    "        'probability': proba\n",
    "    }).sort_values('probability', ascending=False).reset_index(drop=True)\n",
    "\n",
    "def display_all_predictions():\n",
    "    \"\"\"全予測表示\"\"\"\n",
    "    for name in ['all', 'jug', 'non_jug']:\n",
    "        if name not in all_logreg_models:\n",
    "            continue\n",
    "        \n",
    "        for event_type, model_info in sorted(all_logreg_models[name].items()):\n",
    "            pred = predict_next_event(histories[name], event_type, model_info)\n",
    "            if pred is None:\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n{'='*100}\")\n",
    "            print(f\"【次回 {event_type.upper()} 予測 - {name.upper()}】\")\n",
    "            print(f\"{'='*100}\\n\")\n",
    "            print(f\"{'順位':<6} {'末尾':<12} {'確率':>10}\")\n",
    "            print(\"-\" * 30)\n",
    "            for idx, row in pred.iterrows():\n",
    "                print(f\"{idx+1:<6} {row['digit']:<12} {row['probability']:>10.3f}\")\n",
    "\n",
    "display_all_predictions()\n",
    "\n",
    "print(\"\\n✅ 次回イベント予測完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# セル8: 特徴量重要度分析（1回実行）\n",
    "# ============================================================\n",
    "\n",
    "def analyze_feature_importance_all(model_dict, history_df, model_name):\n",
    "    \"\"\"全イベント特徴量重要度分析\"\"\"\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"【{model_name} - 特徴量重要度】\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    for event, model_info in sorted(model_dict.items()):\n",
    "        event_history = history_df[history_df['event_type'] == event]\n",
    "        latest_date = event_history['date'].max()\n",
    "        X_sample, _, _ = prepare_enhanced_features(history_df, event, latest_date)\n",
    "        \n",
    "        if X_sample is None:\n",
    "            continue\n",
    "        \n",
    "        coefficients = model_info['model'].coef_[0]\n",
    "        importance = pd.DataFrame({\n",
    "            'feature': X_sample.columns,\n",
    "            'coefficient': coefficients,\n",
    "            'abs_coefficient': np.abs(coefficients)\n",
    "        }).sort_values('abs_coefficient', ascending=False)\n",
    "        \n",
    "        print(f\"\\n{event.upper()}:\")\n",
    "        print(f\"{'順位':<6} {'特徴量':<35} {'係数':>12} {'絶対値':>12}\")\n",
    "        print(\"-\" * 70)\n",
    "        for idx, (_, row) in enumerate(importance.head(15).iterrows(), 1):\n",
    "            print(f\"{idx:<6} {row['feature']:<35} {row['coefficient']:>12.4f} {row['abs_coefficient']:>12.4f}\")\n",
    "\n",
    "# 全モデル実行\n",
    "for name in ['all', 'jug', 'non_jug']:\n",
    "    if name in all_logreg_models and len(all_logreg_models[name]) > 0:\n",
    "        analyze_feature_importance_all(all_logreg_models[name], histories[name], name.upper())\n",
    "\n",
    "print(\"\\n✅ 特徴量重要度分析完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 特徴量エンジニアリング実験セル\n",
    "# ============================================================\n",
    "\n",
    "def create_enhanced_history_features(df):\n",
    "    \"\"\"拡張特徴量付き履歴作成（ランク追加）\"\"\"\n",
    "    all_dates = sorted(df['date'].unique())\n",
    "    \n",
    "    print(\"  インデックス構築中...\")\n",
    "    event_digit_index = {}\n",
    "    \n",
    "    for date in all_dates:\n",
    "        date_mask = df['date'] == date\n",
    "        date_data = df[date_mask].iloc[0]\n",
    "        \n",
    "        active_events = []\n",
    "        for col, label in EVENT_DEFINITIONS.items():\n",
    "            if col in df.columns and date_data.get(col, 0) == 1:\n",
    "                active_events.append(label.lower())\n",
    "        \n",
    "        for digit in df[date_mask]['last_digit'].unique():\n",
    "            for event_name in active_events:\n",
    "                key = (event_name, digit)\n",
    "                if key not in event_digit_index:\n",
    "                    event_digit_index[key] = []\n",
    "                event_digit_index[key].append(date)\n",
    "    \n",
    "    # キャッシュ作成\n",
    "    diff_cache = df.set_index(['date', 'last_digit'])['avg_diff_coins'].to_dict()\n",
    "    games_cache = df.set_index(['date', 'last_digit'])['avg_games'].to_dict()\n",
    "    \n",
    "    # ランクキャッシュ（利用可能なら）\n",
    "    rank_diff_cache = {}\n",
    "    rank_games_cache = {}\n",
    "    if 'last_digit_rank_diff' in df.columns:\n",
    "        rank_diff_cache = df.set_index(['date', 'last_digit'])['last_digit_rank_diff'].to_dict()\n",
    "    if 'last_digit_rank_games' in df.columns:\n",
    "        rank_games_cache = df.set_index(['date', 'last_digit'])['last_digit_rank_games'].to_dict()\n",
    "    \n",
    "    # 日付×末尾のソート済みリスト\n",
    "    date_digit_pairs = sorted(set((row['date'], row['last_digit']) for _, row in df.iterrows()))\n",
    "    date_digit_index = {}\n",
    "    for date, digit in date_digit_pairs:\n",
    "        if digit not in date_digit_index:\n",
    "            date_digit_index[digit] = []\n",
    "        date_digit_index[digit].append(date)\n",
    "    \n",
    "    print(\"  拡張特徴量生成中...\")\n",
    "    history_features = []\n",
    "    \n",
    "    for (event_name, digit), dates in event_digit_index.items():\n",
    "        for i, date in enumerate(dates):\n",
    "            current_diff = diff_cache.get((date, digit))\n",
    "            current_games = games_cache.get((date, digit))\n",
    "            \n",
    "            if current_diff is None or current_games is None:\n",
    "                continue\n",
    "            \n",
    "            # イベント依存（過去同イベント）\n",
    "            prev_diffs = [diff_cache.get((dates[i-j], digit)) for j in range(1, 4) if i >= j]\n",
    "            prev_games = [games_cache.get((dates[i-j], digit)) for j in range(1, 4) if i >= j]\n",
    "            prev_diffs = [v for v in prev_diffs if v is not None]\n",
    "            prev_games = [v for v in prev_games if v is not None]\n",
    "            \n",
    "            # イベント非依存（直近7日）\n",
    "            digit_history = date_digit_index.get(digit, [])\n",
    "            current_idx = digit_history.index(date) if date in digit_history else -1\n",
    "            \n",
    "            recent_diffs = []\n",
    "            recent_games = []\n",
    "            recent_ranks_diff = []\n",
    "            recent_ranks_games = []\n",
    "            \n",
    "            if current_idx > 0:\n",
    "                for j in range(1, min(8, current_idx + 1)):\n",
    "                    recent_date = digit_history[current_idx - j]\n",
    "                    rd = diff_cache.get((recent_date, digit))\n",
    "                    rg = games_cache.get((recent_date, digit))\n",
    "                    rrd = rank_diff_cache.get((recent_date, digit))\n",
    "                    rrg = rank_games_cache.get((recent_date, digit))\n",
    "                    \n",
    "                    if rd is not None:\n",
    "                        recent_diffs.append(rd)\n",
    "                    if rg is not None:\n",
    "                        recent_games.append(rg)\n",
    "                    if rrd is not None:\n",
    "                        recent_ranks_diff.append(rrd)\n",
    "                    if rrg is not None:\n",
    "                        recent_ranks_games.append(rrg)\n",
    "            \n",
    "            features = {\n",
    "                'date': date, 'last_digit': digit, 'event_type': event_name,\n",
    "                'current_diff': current_diff,\n",
    "                \n",
    "                # イベント依存：基本\n",
    "                'prev_1_diff': prev_diffs[0] if len(prev_diffs) > 0 else 0,\n",
    "                'prev_1_games': prev_games[0] if len(prev_games) > 0 else 0,\n",
    "                \n",
    "                # 直近履歴：差枚・ゲーム数\n",
    "                'recent_mean_diff': np.mean(recent_diffs) if len(recent_diffs) > 0 else 0,\n",
    "                'recent_max_diff': np.max(recent_diffs) if len(recent_diffs) > 0 else 0,\n",
    "                'recent_mean_games': np.mean(recent_games) if len(recent_games) > 0 else 0,\n",
    "                'recent_max_games': np.max(recent_games) if len(recent_games) > 0 else 0,\n",
    "                \n",
    "                # 直近履歴：ランク\n",
    "                'recent_mean_rank_diff': np.mean(recent_ranks_diff) if len(recent_ranks_diff) > 0 else 0,\n",
    "                'recent_best_rank_diff': np.min(recent_ranks_diff) if len(recent_ranks_diff) > 0 else 0,\n",
    "                'recent_mean_rank_games': np.mean(recent_ranks_games) if len(recent_ranks_games) > 0 else 0,\n",
    "                'recent_best_rank_games': np.min(recent_ranks_games) if len(recent_ranks_games) > 0 else 0,\n",
    "                \n",
    "                # ランク安定性（標準偏差が小さい=安定上位）\n",
    "                'rank_diff_stability': np.std(recent_ranks_diff) if len(recent_ranks_diff) > 1 else 0,\n",
    "                'rank_games_stability': np.std(recent_ranks_games) if len(recent_ranks_games) > 1 else 0,\n",
    "                \n",
    "                # TOP3率（直近7日でランク3位以内の割合）\n",
    "                'top3_rate_diff': sum(1 for r in recent_ranks_diff if r <= 3) / len(recent_ranks_diff) if len(recent_ranks_diff) > 0 else 0,\n",
    "                'top3_rate_games': sum(1 for r in recent_ranks_games if r <= 3) / len(recent_ranks_games) if len(recent_ranks_games) > 0 else 0,\n",
    "                \n",
    "                # データ充実度\n",
    "                'data_count': len(recent_diffs),\n",
    "            }\n",
    "            \n",
    "            history_features.append(features)\n",
    "    \n",
    "    return pd.DataFrame(history_features)\n",
    "\n",
    "def prepare_features_flexible(history_df, event_type, cutoff_date, include_current=False, \n",
    "                               feature_set='basic'):\n",
    "    \"\"\"柔軟な特徴量選択\"\"\"\n",
    "    if include_current:\n",
    "        event_data = history_df[\n",
    "            (history_df['event_type'] == event_type) & \n",
    "            (history_df['date'] <= cutoff_date)\n",
    "        ].copy()\n",
    "    else:\n",
    "        event_data = history_df[\n",
    "            (history_df['event_type'] == event_type) & \n",
    "            (history_df['date'] < cutoff_date)\n",
    "        ].copy()\n",
    "    \n",
    "    if len(event_data) == 0:\n",
    "        return None, None, None\n",
    "    \n",
    "    # 特徴量セット定義\n",
    "    feature_sets = {\n",
    "        'basic': ['prev_1_diff', 'prev_1_games'],\n",
    "        \n",
    "        'rank_focused': ['recent_mean_rank_diff', 'recent_best_rank_diff', 'top3_rate_diff',\n",
    "                        'recent_mean_rank_games', 'top3_rate_games'],\n",
    "        \n",
    "        'rank_and_perf': ['recent_mean_diff', 'recent_max_games', \n",
    "                         'recent_mean_rank_diff', 'recent_best_rank_games', 'top3_rate_diff'],\n",
    "        \n",
    "        'all': ['prev_1_diff', 'prev_1_games',\n",
    "               'recent_mean_diff', 'recent_max_diff', 'recent_mean_games', 'recent_max_games',\n",
    "               'recent_mean_rank_diff', 'recent_best_rank_diff', 'recent_mean_rank_games', 'recent_best_rank_games',\n",
    "               'rank_diff_stability', 'rank_games_stability', 'top3_rate_diff', 'top3_rate_games', 'data_count']\n",
    "    }\n",
    "    \n",
    "    selected_features = [f for f in feature_sets[feature_set] if f in event_data.columns]\n",
    "    X = event_data[selected_features].fillna(0)\n",
    "    y = (event_data['current_diff'] > event_data['current_diff'].median()).astype(int)\n",
    "    \n",
    "    return X, y, event_data\n",
    "\n",
    "def compare_feature_sets(history_df, model_name, n_test=5):\n",
    "    \"\"\"複数特徴量セットの比較\"\"\"\n",
    "    feature_sets = ['basic', 'rank_focused', 'rank_and_perf', 'all']\n",
    "    results_summary = []\n",
    "    \n",
    "    for feature_set in feature_sets:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"【{feature_set.upper()} 特徴量セット - {model_name}】\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        set_results = []\n",
    "        \n",
    "        for event_type in sorted(history_df['event_type'].unique())[:3]:  # 上位3イベントで検証\n",
    "            event_history = history_df[history_df['event_type'] == event_type].copy()\n",
    "            event_dates = sorted(event_history['date'].unique())\n",
    "            \n",
    "            if len(event_dates) < n_test + 3:\n",
    "                continue\n",
    "            \n",
    "            test_dates = event_dates[-n_test:]\n",
    "            precisions = []\n",
    "            \n",
    "            for test_date in test_dates:\n",
    "                train_cutoff = event_dates[event_dates.index(test_date) - 1]\n",
    "                X_train, y_train, _ = prepare_features_flexible(\n",
    "                    history_df, event_type, train_cutoff, include_current=True, feature_set=feature_set\n",
    "                )\n",
    "                \n",
    "                if X_train is None or len(X_train) == 0 or X_train.shape[1] == 0:\n",
    "                    print(f\"  スキップ: {event_type}/{test_date} - 特徴量なし\")\n",
    "                    continue\n",
    "                \n",
    "                test_data = event_history[event_history['date'] == test_date]\n",
    "                X_test_list = []\n",
    "                \n",
    "                for digit in test_data['last_digit'].values:\n",
    "                    digit_history = history_df[\n",
    "                        (history_df['event_type'] == event_type) & \n",
    "                        (history_df['last_digit'] == digit) &\n",
    "                        (history_df['date'] < test_date)\n",
    "                    ]\n",
    "                    if len(digit_history) > 0:\n",
    "                        latest = digit_history.iloc[-1]\n",
    "                        X_test_list.append([latest.get(f, 0) for f in X_train.columns])\n",
    "                    else:\n",
    "                        X_test_list.append([0] * len(X_train.columns))\n",
    "                \n",
    "                X_test = pd.DataFrame(X_test_list, columns=X_train.columns)\n",
    "                \n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "                \n",
    "                model = LogisticRegression(C=1.0, class_weight='balanced', max_iter=1000, random_state=42)\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                \n",
    "                proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "                predicted_top3 = [d for d, _ in sorted(zip(test_data['last_digit'].values, proba), \n",
    "                                                        key=lambda x: x[1], reverse=True)[:3]]\n",
    "                actual_top3 = test_data.nlargest(3, 'current_diff')['last_digit'].tolist()\n",
    "                overlap = len(set(predicted_top3) & set(actual_top3))\n",
    "                precisions.append(overlap / 3)\n",
    "            \n",
    "            if len(precisions) > 0:\n",
    "                set_results.append({\n",
    "                    'event': event_type,\n",
    "                    'precision': np.mean(precisions),\n",
    "                    'n_features': len(X_train.columns)\n",
    "                })\n",
    "        \n",
    "        if len(set_results) > 0:\n",
    "            avg_precision = np.mean([r['precision'] for r in set_results])\n",
    "            avg_n_features = np.mean([r['n_features'] for r in set_results])\n",
    "            \n",
    "            print(f\"\\n平均 Precision@3: {avg_precision:.3f}\")\n",
    "            print(f\"平均特徴量数: {avg_n_features:.0f}\")\n",
    "            \n",
    "            results_summary.append({\n",
    "                'feature_set': feature_set,\n",
    "                'model': model_name,\n",
    "                'avg_precision': avg_precision,\n",
    "                'avg_n_features': avg_n_features\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results_summary)\n",
    "\n",
    "# 実行\n",
    "print(\"拡張特徴量履歴作成中...\")\n",
    "enhanced_histories = {}\n",
    "for name, df in [('all', df_all), ('jug', df_jug), ('non_jug', df_non_jug)]:\n",
    "    if len(df) > 0:\n",
    "        print(f\"\\n【{name.upper()}】\")\n",
    "        enhanced_histories[name] = create_enhanced_history_features(df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"特徴量セット比較実験\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "comparison_results = []\n",
    "for name in ['all', 'jug', 'non_jug']:\n",
    "    if name in enhanced_histories and len(enhanced_histories[name]) > 0:\n",
    "        result = compare_feature_sets(enhanced_histories[name], name.upper(), n_test=5)\n",
    "        comparison_results.append(result)\n",
    "\n",
    "# サマリー表示\n",
    "if len(comparison_results) > 0:\n",
    "    final_summary = pd.concat(comparison_results, ignore_index=True)\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"【総合結果サマリー】\")\n",
    "    print(\"=\"*100)\n",
    "    print(final_summary.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n【ベスト特徴量セット】\")\n",
    "    best = final_summary.loc[final_summary.groupby('model')['avg_precision'].idxmax()]\n",
    "    print(best[['model', 'feature_set', 'avg_precision', 'avg_n_features']].to_string(index=False))\n",
    "\n",
    "print(\"\\n✅ 特徴量エンジニアリング実験完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# XGBoost/RandomForest特徴量エンジニアリング実験\n",
    "# ============================================================\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def compare_tree_models(history_df, model_name, n_test=5):\n",
    "    \"\"\"XGBoost/RandomForest/LogRegの3モデル比較\"\"\"\n",
    "    feature_sets = ['basic', 'rank_focused', 'rank_and_perf', 'all']\n",
    "    results_summary = []\n",
    "    \n",
    "    for feature_set in feature_sets:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"【{feature_set.upper()} - {model_name}】\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for event_type in sorted(history_df['event_type'].unique())[:3]:\n",
    "            event_history = history_df[history_df['event_type'] == event_type].copy()\n",
    "            event_dates = sorted(event_history['date'].unique())\n",
    "            \n",
    "            if len(event_dates) < n_test + 3:\n",
    "                continue\n",
    "            \n",
    "            test_dates = event_dates[-n_test:]\n",
    "            \n",
    "            # 3モデルの結果格納\n",
    "            model_results = {'LogReg': [], 'XGB': [], 'RF': []}\n",
    "            \n",
    "            for test_date in test_dates:\n",
    "                train_cutoff = event_dates[event_dates.index(test_date) - 1]\n",
    "                X_train, y_train, _ = prepare_features_flexible(\n",
    "                    history_df, event_type, train_cutoff, include_current=True, feature_set=feature_set\n",
    "                )\n",
    "                \n",
    "                if X_train is None or len(X_train) == 0 or X_train.shape[1] == 0:\n",
    "                    continue\n",
    "                \n",
    "                test_data = event_history[event_history['date'] == test_date]\n",
    "                X_test_list = []\n",
    "                \n",
    "                for digit in test_data['last_digit'].values:\n",
    "                    digit_history = history_df[\n",
    "                        (history_df['event_type'] == event_type) & \n",
    "                        (history_df['last_digit'] == digit) &\n",
    "                        (history_df['date'] < test_date)\n",
    "                    ]\n",
    "                    if len(digit_history) > 0:\n",
    "                        latest = digit_history.iloc[-1]\n",
    "                        X_test_list.append([latest.get(f, 0) for f in X_train.columns])\n",
    "                    else:\n",
    "                        X_test_list.append([0] * len(X_train.columns))\n",
    "                \n",
    "                X_test = pd.DataFrame(X_test_list, columns=X_train.columns)\n",
    "                \n",
    "                # 正解データ\n",
    "                actual_top3 = test_data.nlargest(3, 'current_diff')['last_digit'].tolist()\n",
    "                \n",
    "                # LogisticRegression\n",
    "                scaler_lr = StandardScaler()\n",
    "                X_train_lr = scaler_lr.fit_transform(X_train)\n",
    "                X_test_lr = scaler_lr.transform(X_test)\n",
    "                \n",
    "                lr = LogisticRegression(C=1.0, class_weight='balanced', max_iter=1000, random_state=42)\n",
    "                lr.fit(X_train_lr, y_train)\n",
    "                proba_lr = lr.predict_proba(X_test_lr)[:, 1]\n",
    "                pred_lr = [d for d, _ in sorted(zip(test_data['last_digit'].values, proba_lr), \n",
    "                                                key=lambda x: x[1], reverse=True)[:3]]\n",
    "                model_results['LogReg'].append(len(set(pred_lr) & set(actual_top3)) / 3)\n",
    "                \n",
    "                # XGBoost\n",
    "                xgb = XGBClassifier(\n",
    "                    n_estimators=100, max_depth=5, learning_rate=0.1,\n",
    "                    subsample=0.8, colsample_bytree=0.8,\n",
    "                    random_state=42, eval_metric='logloss'\n",
    "                )\n",
    "                xgb.fit(X_train, y_train)\n",
    "                proba_xgb = xgb.predict_proba(X_test)[:, 1]\n",
    "                pred_xgb = [d for d, _ in sorted(zip(test_data['last_digit'].values, proba_xgb), \n",
    "                                                 key=lambda x: x[1], reverse=True)[:3]]\n",
    "                model_results['XGB'].append(len(set(pred_xgb) & set(actual_top3)) / 3)\n",
    "                \n",
    "                # RandomForest\n",
    "                rf = RandomForestClassifier(\n",
    "                    n_estimators=100, max_depth=10, min_samples_split=5,\n",
    "                    random_state=42, class_weight='balanced', n_jobs=-1\n",
    "                )\n",
    "                rf.fit(X_train, y_train)\n",
    "                proba_rf = rf.predict_proba(X_test)[:, 1]\n",
    "                pred_rf = [d for d, _ in sorted(zip(test_data['last_digit'].values, proba_rf), \n",
    "                                                key=lambda x: x[1], reverse=True)[:3]]\n",
    "                model_results['RF'].append(len(set(pred_rf) & set(actual_top3)) / 3)\n",
    "            \n",
    "            if len(model_results['LogReg']) > 0:\n",
    "                avg_lr = np.mean(model_results['LogReg'])\n",
    "                avg_xgb = np.mean(model_results['XGB'])\n",
    "                avg_rf = np.mean(model_results['RF'])\n",
    "                \n",
    "                print(f\"\\n{event_type}:\")\n",
    "                print(f\"  LogReg: {avg_lr:.3f}\")\n",
    "                print(f\"  XGBoost: {avg_xgb:.3f}\")\n",
    "                print(f\"  RandomForest: {avg_rf:.3f}\")\n",
    "                \n",
    "                results_summary.append({\n",
    "                    'feature_set': feature_set,\n",
    "                    'model_type': model_name,\n",
    "                    'event': event_type,\n",
    "                    'LogReg': avg_lr,\n",
    "                    'XGB': avg_xgb,\n",
    "                    'RF': avg_rf,\n",
    "                    'n_features': len(X_train.columns)\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results_summary)\n",
    "\n",
    "# 実行\n",
    "print(\"=\"*100)\n",
    "print(\"XGBoost/RandomForest 特徴量エンジニアリング実験\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "tree_comparison_results = []\n",
    "for name in ['all', 'jug', 'non_jug']:\n",
    "    if name in enhanced_histories and len(enhanced_histories[name]) > 0:\n",
    "        result = compare_tree_models(enhanced_histories[name], name.upper(), n_test=5)\n",
    "        tree_comparison_results.append(result)\n",
    "\n",
    "# サマリー表示\n",
    "if len(tree_comparison_results) > 0:\n",
    "    final_tree_summary = pd.concat(tree_comparison_results, ignore_index=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"【総合結果サマリー】\")\n",
    "    print(\"=\"*100)\n",
    "    print(final_tree_summary.to_string(index=False))\n",
    "    \n",
    "    # モデル別平均\n",
    "    print(\"\\n【モデル別平均Precision@3】\")\n",
    "    for model_type in final_tree_summary['model_type'].unique():\n",
    "        subset = final_tree_summary[final_tree_summary['model_type'] == model_type]\n",
    "        print(f\"\\n{model_type}:\")\n",
    "        print(f\"  LogReg: {subset['LogReg'].mean():.3f}\")\n",
    "        print(f\"  XGBoost: {subset['XGB'].mean():.3f}\")\n",
    "        print(f\"  RandomForest: {subset['RF'].mean():.3f}\")\n",
    "    \n",
    "    # ベスト組み合わせ\n",
    "    print(\"\\n【ベストモデル×特徴量セット】\")\n",
    "    for model_type in final_tree_summary['model_type'].unique():\n",
    "        subset = final_tree_summary[final_tree_summary['model_type'] == model_type]\n",
    "        \n",
    "        best_lr = subset.loc[subset['LogReg'].idxmax()]\n",
    "        best_xgb = subset.loc[subset['XGB'].idxmax()]\n",
    "        best_rf = subset.loc[subset['RF'].idxmax()]\n",
    "        \n",
    "        print(f\"\\n{model_type}:\")\n",
    "        print(f\"  LogReg best: {best_lr['feature_set']} ({best_lr['LogReg']:.3f})\")\n",
    "        print(f\"  XGBoost best: {best_xgb['feature_set']} ({best_xgb['XGB']:.3f})\")\n",
    "        print(f\"  RandomForest best: {best_rf['feature_set']} ({best_rf['RF']:.3f})\")\n",
    "\n",
    "print(\"\\n✅ ツリーモデル比較実験完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TPOT AutoML実験（安定化・Daskローカル対応版）\n",
    "# ============================================================\n",
    "\n",
    "# !pip install tpot dask distributed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "# Daskローカルクライアント起動（ワーカー不在エラー回避）\n",
    "try:\n",
    "    from dask.distributed import Client\n",
    "    client = Client(processes=False, n_workers=2, threads_per_worker=2)\n",
    "    print(client)\n",
    "except Exception as e:\n",
    "    client = None\n",
    "    print(f\"Dask client not started: {e}\")\n",
    "\n",
    "def run_tpot_automl(history_df, model_name, event_types=None, n_test=2):\n",
    "    \"\"\"TPOT AutoML実行（互換・ガード強化版）\"\"\"\n",
    "    if event_types is None:\n",
    "        if 'event_type' not in history_df.columns:\n",
    "            print(f\"[{model_name}] 'event_type' 列が見つかりません。処理をスキップします。\")\n",
    "            return pd.DataFrame()\n",
    "        event_types = sorted(history_df['event_type'].unique())[:2]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for ev in event_types:\n",
    "        df_ev = history_df[history_df['event_type'] == ev].copy()\n",
    "        if len(df_ev) < 40:\n",
    "            print(f\"[{model_name}:{ev}] skip: samples too small ({len(df_ev)})\")\n",
    "            continue\n",
    "\n",
    "        # 時系列順（date, last_digitがあれば）で並べる\n",
    "        sort_cols = [c for c in ['date', 'last_digit'] if c in df_ev.columns]\n",
    "        if sort_cols:\n",
    "            df_ev = df_ev.sort_values(sort_cols).reset_index(drop=True)\n",
    "\n",
    "        # 特徴量候補\n",
    "        available_features = [\n",
    "            c for c in df_ev.columns\n",
    "            if c.startswith(('prev_', 'digit_num', 'is_', 'weekday_num', 'day_of_month'))\n",
    "        ]\n",
    "        if not available_features:\n",
    "            print(f\"[{model_name}:{ev}] skip: no available features\")\n",
    "            continue\n",
    "\n",
    "        # テスト期間（末尾 n_test 日）\n",
    "        if 'date' not in df_ev.columns:\n",
    "            print(f\"[{model_name}:{ev}] skip: 'date' 列なし\")\n",
    "            continue\n",
    "\n",
    "        unique_dates = sorted(df_ev['date'].unique())\n",
    "        if len(unique_dates) <= n_test:\n",
    "            print(f\"[{model_name}:{ev}] skip: not enough dates for test split\")\n",
    "            continue\n",
    "\n",
    "        test_dates = set(unique_dates[-n_test:])\n",
    "        train_data = df_ev[~df_ev['date'].isin(test_dates)].copy()\n",
    "        test_data  = df_ev[df_ev['date'].isin(test_dates)].copy()\n",
    "\n",
    "        # 前処理: NaN/inf -> 0\n",
    "        def clean(X):\n",
    "            X = X.replace([np.inf, -np.inf], 0)\n",
    "            return X.fillna(0)\n",
    "\n",
    "        X_train = clean(train_data[available_features])\n",
    "        X_test  = clean(test_data[available_features])\n",
    "\n",
    "        # ラベル: median超で1（既存仕様踏襲）\n",
    "        if 'current_diff' not in train_data.columns or 'current_diff' not in test_data.columns:\n",
    "            print(f\"[{model_name}:{ev}] skip: 'current_diff' 列なし\")\n",
    "            continue\n",
    "\n",
    "        y_train = (train_data['current_diff'] > train_data['current_diff'].median()).astype(int).values\n",
    "\n",
    "        # 単一クラスをスキップ\n",
    "        if len(np.unique(y_train)) < 2:\n",
    "            print(f\"[{model_name}:{ev}] skip: y_train single class (counts={np.bincount(y_train) if y_train.size else []})\")\n",
    "            continue\n",
    "\n",
    "        # TPOT 初期化（バージョン互換のため最小引数のみ、Daskクライアントを渡す）\n",
    "        def build_tpot(with_client=True):\n",
    "            common = dict(\n",
    "                generations=2,          # 軽め\n",
    "                population_size=10,     # 軽め\n",
    "                cv=3,                   # 互換性のため整数指定\n",
    "                n_jobs=-1,\n",
    "                verbose=2,              # 旧: verbosity\n",
    "                random_state=42,\n",
    "                max_time_mins=10,\n",
    "                max_eval_time_mins=2\n",
    "            )\n",
    "            if with_client and client is not None:\n",
    "                return TPOTClassifier(client=client, **common)\n",
    "            else:\n",
    "                return TPOTClassifier(**common)\n",
    "\n",
    "        try:\n",
    "            try:\n",
    "                tpot = build_tpot(with_client=True)\n",
    "                tpot.fit(X_train, y_train)\n",
    "            except TypeError:\n",
    "                # 一部版で client 引数が未対応な場合のフォールバック\n",
    "                tpot = build_tpot(with_client=False)\n",
    "                tpot.fit(X_train, y_train)\n",
    "        except Exception as e:\n",
    "            print(f\"[{model_name}:{ev}] TPOT fit error: {e}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # 予測（predict_proba未対応の器に備えフォールバック）\n",
    "        try:\n",
    "            proba = tpot.predict_proba(X_test)[:, 1]\n",
    "        except Exception:\n",
    "            proba = tpot.predict(X_test).astype(float)\n",
    "\n",
    "        if 'last_digit' not in test_data.columns:\n",
    "            print(f\"[{model_name}:{ev}] warning: 'last_digit' 列なし（Top3計算をスキップ）\")\n",
    "            continue\n",
    "\n",
    "        # Top3（確率上位3の末尾）\n",
    "        predicted_top3 = [\n",
    "            d for d, _ in sorted(\n",
    "                zip(test_data['last_digit'].values, proba),\n",
    "                key=lambda x: x[1],\n",
    "                reverse=True\n",
    "            )[:3]\n",
    "        ]\n",
    "        actual_top3 = [\n",
    "            d for d, _ in sorted(\n",
    "                zip(test_data['last_digit'].values, test_data['current_diff'].values),\n",
    "                key=lambda x: x[1],\n",
    "                reverse=True\n",
    "            )[:3]\n",
    "        ]\n",
    "        overlap = len(set(predicted_top3) & set(actual_top3))\n",
    "\n",
    "        results.append({\n",
    "            'event': ev,\n",
    "            'precision': overlap / 3.0,\n",
    "            'predicted': predicted_top3,\n",
    "            'actual': actual_top3\n",
    "        })\n",
    "\n",
    "        # 参考出力\n",
    "        try:\n",
    "            print(f\"ベストパイプライン: {type(tpot.fitted_pipeline_).__name__}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "        print(f\"Precision@3: {overlap/3:.3f}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# 実行\n",
    "print(\"=\"*100)\n",
    "print(\"TPOT AutoML実験\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "tpot_results = []\n",
    "for name in ['all']:  # 時間削減のため1モデルのみ\n",
    "    if name in enhanced_histories and len(enhanced_histories[name]) > 0:\n",
    "        result = run_tpot_automl(enhanced_histories[name], name.upper(), n_test=2)\n",
    "        if result is not None and not result.empty:\n",
    "            tpot_results.append(result)\n",
    "\n",
    "# サマリー（安全化）\n",
    "if len(tpot_results) > 0 and any((len(df) > 0 for df in tpot_results)):\n",
    "    final_tpot = pd.concat(tpot_results, ignore_index=True)\n",
    "    if 'precision' in final_tpot.columns and not final_tpot.empty:\n",
    "        print(f\"\\n平均Precision@3: {final_tpot['precision'].mean():.3f}\")\n",
    "        print(\"\\n詳細:\")\n",
    "        cols = [c for c in ['event', 'precision', 'predicted', 'actual'] if c in final_tpot.columns]\n",
    "        print(final_tpot[cols].to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\n有効な結果がありません（すべてスキップ/失敗）。時間設定や分割条件を見直してください。\")\n",
    "else:\n",
    "    print(\"\\n結果が空です。\")\n",
    "\n",
    "# Dask後片付け（任意）\n",
    "try:\n",
    "    if client is not None:\n",
    "        client.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"\\n✅ TPOT AutoML完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 特徴量エンジニアリング実験セル\n",
    "# ============================================================\n",
    "\n",
    "def create_enhanced_history_features(df):\n",
    "    \"\"\"拡張特徴量付き履歴作成（イベント×末尾マッチング追加）\"\"\"\n",
    "    all_dates = sorted(df['date'].unique())\n",
    "    \n",
    "    print(\"  インデックス構築中...\")\n",
    "    event_digit_index = {}\n",
    "    \n",
    "    for date in all_dates:\n",
    "        date_mask = df['date'] == date\n",
    "        date_data = df[date_mask].iloc[0]\n",
    "        \n",
    "        active_events = []\n",
    "        for col, label in EVENT_DEFINITIONS.items():\n",
    "            if col in df.columns and date_data.get(col, 0) == 1:\n",
    "                active_events.append(label.lower())\n",
    "        \n",
    "        for digit in df[date_mask]['last_digit'].unique():\n",
    "            for event_name in active_events:\n",
    "                key = (event_name, digit)\n",
    "                if key not in event_digit_index:\n",
    "                    event_digit_index[key] = []\n",
    "                event_digit_index[key].append(date)\n",
    "    \n",
    "    # キャッシュ作成\n",
    "    diff_cache = df.set_index(['date', 'last_digit'])['avg_diff_coins'].to_dict()\n",
    "    games_cache = df.set_index(['date', 'last_digit'])['avg_games'].to_dict()\n",
    "    \n",
    "    # ランクキャッシュ\n",
    "    rank_diff_cache = {}\n",
    "    rank_games_cache = {}\n",
    "    if 'last_digit_rank_diff' in df.columns:\n",
    "        rank_diff_cache = df.set_index(['date', 'last_digit'])['last_digit_rank_diff'].to_dict()\n",
    "    if 'last_digit_rank_games' in df.columns:\n",
    "        rank_games_cache = df.set_index(['date', 'last_digit'])['last_digit_rank_games'].to_dict()\n",
    "    \n",
    "    # 日付×末尾のソート済みリスト\n",
    "    date_digit_pairs = sorted(set((row['date'], row['last_digit']) for _, row in df.iterrows()))\n",
    "    date_digit_index = {}\n",
    "    for date, digit in date_digit_pairs:\n",
    "        if digit not in date_digit_index:\n",
    "            date_digit_index[digit] = []\n",
    "        date_digit_index[digit].append(date)\n",
    "    \n",
    "    print(\"  拡張特徴量生成中...\")\n",
    "    history_features = []\n",
    "    \n",
    "    for (event_name, digit), dates in event_digit_index.items():\n",
    "        # イベント×末尾マッチング判定\n",
    "        is_matching = False\n",
    "        if event_name in ['1day', '2day', '3day', '4day', '5day', '6day', '7day', '8day', '9day', '0day']:\n",
    "            event_digit = event_name[0]  # '7day' → '7'\n",
    "            is_matching = (str(digit) == event_digit)\n",
    "        \n",
    "        # このイベント×末尾の過去実績集計\n",
    "        matching_history_diffs = []\n",
    "        matching_history_ranks = []\n",
    "        \n",
    "        for date in dates:\n",
    "            d = diff_cache.get((date, digit))\n",
    "            r = rank_diff_cache.get((date, digit))\n",
    "            if d is not None:\n",
    "                matching_history_diffs.append(d)\n",
    "            if r is not None:\n",
    "                matching_history_ranks.append(r)\n",
    "        \n",
    "        for i, date in enumerate(dates):\n",
    "            current_diff = diff_cache.get((date, digit))\n",
    "            current_games = games_cache.get((date, digit))\n",
    "            \n",
    "            if current_diff is None or current_games is None:\n",
    "                continue\n",
    "            \n",
    "            # イベント依存（過去同イベント）\n",
    "            prev_diffs = [diff_cache.get((dates[i-j], digit)) for j in range(1, 4) if i >= j]\n",
    "            prev_games = [games_cache.get((dates[i-j], digit)) for j in range(1, 4) if i >= j]\n",
    "            prev_diffs = [v for v in prev_diffs if v is not None]\n",
    "            prev_games = [v for v in prev_games if v is not None]\n",
    "            \n",
    "            # イベント非依存（直近7日）\n",
    "            digit_history = date_digit_index.get(digit, [])\n",
    "            current_idx = digit_history.index(date) if date in digit_history else -1\n",
    "            \n",
    "            recent_diffs = []\n",
    "            recent_games = []\n",
    "            recent_ranks_diff = []\n",
    "            recent_ranks_games = []\n",
    "            \n",
    "            if current_idx > 0:\n",
    "                for j in range(1, min(8, current_idx + 1)):\n",
    "                    recent_date = digit_history[current_idx - j]\n",
    "                    rd = diff_cache.get((recent_date, digit))\n",
    "                    rg = games_cache.get((recent_date, digit))\n",
    "                    rrd = rank_diff_cache.get((recent_date, digit))\n",
    "                    rrg = rank_games_cache.get((recent_date, digit))\n",
    "                    \n",
    "                    if rd is not None:\n",
    "                        recent_diffs.append(rd)\n",
    "                    if rg is not None:\n",
    "                        recent_games.append(rg)\n",
    "                    if rrd is not None:\n",
    "                        recent_ranks_diff.append(rrd)\n",
    "                    if rrg is not None:\n",
    "                        recent_ranks_games.append(rrg)\n",
    "            \n",
    "            # マッチング特徴量（過去のこのイベント×末尾の実績、当日除外）\n",
    "            past_matching_diffs = matching_history_diffs[:i] if i > 0 else []\n",
    "            past_matching_ranks = matching_history_ranks[:i] if i > 0 else []\n",
    "            \n",
    "            features = {\n",
    "                'date': date, 'last_digit': digit, 'event_type': event_name,\n",
    "                'current_diff': current_diff,\n",
    "                \n",
    "                # イベント依存：基本\n",
    "                'prev_1_diff': prev_diffs[0] if len(prev_diffs) > 0 else 0,\n",
    "                'prev_1_games': prev_games[0] if len(prev_games) > 0 else 0,\n",
    "                \n",
    "                # 直近履歴：差枚・ゲーム数\n",
    "                'recent_mean_diff': np.mean(recent_diffs) if len(recent_diffs) > 0 else 0,\n",
    "                'recent_max_diff': np.max(recent_diffs) if len(recent_diffs) > 0 else 0,\n",
    "                'recent_mean_games': np.mean(recent_games) if len(recent_games) > 0 else 0,\n",
    "                'recent_max_games': np.max(recent_games) if len(recent_games) > 0 else 0,\n",
    "                \n",
    "                # 直近履歴：ランク\n",
    "                'recent_mean_rank_diff': np.mean(recent_ranks_diff) if len(recent_ranks_diff) > 0 else 0,\n",
    "                'recent_best_rank_diff': np.min(recent_ranks_diff) if len(recent_ranks_diff) > 0 else 0,\n",
    "                'recent_mean_rank_games': np.mean(recent_ranks_games) if len(recent_ranks_games) > 0 else 0,\n",
    "                'recent_best_rank_games': np.min(recent_ranks_games) if len(recent_ranks_games) > 0 else 0,\n",
    "                \n",
    "                # ランク安定性\n",
    "                'rank_diff_stability': np.std(recent_ranks_diff) if len(recent_ranks_diff) > 1 else 0,\n",
    "                'rank_games_stability': np.std(recent_ranks_games) if len(recent_ranks_games) > 1 else 0,\n",
    "                \n",
    "                # TOP3率\n",
    "                'top3_rate_diff': sum(1 for r in recent_ranks_diff if r <= 3) / len(recent_ranks_diff) if len(recent_ranks_diff) > 0 else 0,\n",
    "                'top3_rate_games': sum(1 for r in recent_ranks_games if r <= 3) / len(recent_ranks_games) if len(recent_ranks_games) > 0 else 0,\n",
    "                \n",
    "                # データ充実度\n",
    "                'data_count': len(recent_diffs),\n",
    "                \n",
    "                # ★新規：イベント×末尾マッチング特徴量\n",
    "                'is_matching_digit': 1 if is_matching else 0,\n",
    "                'matching_history_mean': np.mean(past_matching_diffs) if len(past_matching_diffs) > 0 else 0,\n",
    "                'matching_history_max': np.max(past_matching_diffs) if len(past_matching_diffs) > 0 else 0,\n",
    "                'matching_history_count': len(past_matching_diffs),\n",
    "                'matching_rank_mean': np.mean(past_matching_ranks) if len(past_matching_ranks) > 0 else 0,\n",
    "                'matching_top3_rate': sum(1 for r in past_matching_ranks if r <= 3) / len(past_matching_ranks) if len(past_matching_ranks) > 0 else 0,\n",
    "            }\n",
    "            \n",
    "            history_features.append(features)\n",
    "    \n",
    "    return pd.DataFrame(history_features)\n",
    "\n",
    "def prepare_features_flexible(history_df, event_type, cutoff_date, include_current=False, \n",
    "                               feature_set='basic'):\n",
    "    \"\"\"柔軟な特徴量選択\"\"\"\n",
    "    if include_current:\n",
    "        event_data = history_df[\n",
    "            (history_df['event_type'] == event_type) & \n",
    "            (history_df['date'] <= cutoff_date)\n",
    "        ].copy()\n",
    "    else:\n",
    "        event_data = history_df[\n",
    "            (history_df['event_type'] == event_type) & \n",
    "            (history_df['date'] < cutoff_date)\n",
    "        ].copy()\n",
    "    \n",
    "    if len(event_data) == 0:\n",
    "        return None, None, None\n",
    "    \n",
    "    # 特徴量セット定義\n",
    "    feature_sets = {\n",
    "        'basic': ['prev_1_diff', 'prev_1_games'],\n",
    "        \n",
    "        'rank_focused': ['recent_mean_rank_diff', 'recent_best_rank_diff', 'top3_rate_diff',\n",
    "                        'recent_mean_rank_games', 'top3_rate_games'],\n",
    "        \n",
    "        'rank_and_perf': ['recent_mean_diff', 'recent_max_games', \n",
    "                         'recent_mean_rank_diff', 'recent_best_rank_games', 'top3_rate_diff'],\n",
    "        \n",
    "        'all': ['prev_1_diff', 'prev_1_games',\n",
    "               'recent_mean_diff', 'recent_max_diff', 'recent_mean_games', 'recent_max_games',\n",
    "               'recent_mean_rank_diff', 'recent_best_rank_diff', 'recent_mean_rank_games', 'recent_best_rank_games',\n",
    "               'rank_diff_stability', 'rank_games_stability', 'top3_rate_diff', 'top3_rate_games', 'data_count']\n",
    "    }\n",
    "    \n",
    "    selected_features = [f for f in feature_sets[feature_set] if f in event_data.columns]\n",
    "    X = event_data[selected_features].fillna(0)\n",
    "    y = (event_data['current_diff'] > event_data['current_diff'].median()).astype(int)\n",
    "    \n",
    "    return X, y, event_data\n",
    "\n",
    "def compare_feature_sets(history_df, model_name, n_test=5):\n",
    "    \"\"\"複数特徴量セットの比較\"\"\"\n",
    "    feature_sets = ['basic', 'rank_focused', 'rank_and_perf', 'all']\n",
    "    results_summary = []\n",
    "    \n",
    "    for feature_set in feature_sets:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"【{feature_set.upper()} 特徴量セット - {model_name}】\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        set_results = []\n",
    "        \n",
    "        for event_type in sorted(history_df['event_type'].unique())[:3]:  # 上位3イベントで検証\n",
    "            event_history = history_df[history_df['event_type'] == event_type].copy()\n",
    "            event_dates = sorted(event_history['date'].unique())\n",
    "            \n",
    "            if len(event_dates) < n_test + 3:\n",
    "                continue\n",
    "            \n",
    "            test_dates = event_dates[-n_test:]\n",
    "            precisions = []\n",
    "            \n",
    "            for test_date in test_dates:\n",
    "                train_cutoff = event_dates[event_dates.index(test_date) - 1]\n",
    "                X_train, y_train, _ = prepare_features_flexible(\n",
    "                    history_df, event_type, train_cutoff, include_current=True, feature_set=feature_set\n",
    "                )\n",
    "                \n",
    "                if X_train is None or len(X_train) == 0 or X_train.shape[1] == 0:\n",
    "                    print(f\"  スキップ: {event_type}/{test_date} - 特徴量なし\")\n",
    "                    continue\n",
    "                \n",
    "                test_data = event_history[event_history['date'] == test_date]\n",
    "                X_test_list = []\n",
    "                \n",
    "                for digit in test_data['last_digit'].values:\n",
    "                    digit_history = history_df[\n",
    "                        (history_df['event_type'] == event_type) & \n",
    "                        (history_df['last_digit'] == digit) &\n",
    "                        (history_df['date'] < test_date)\n",
    "                    ]\n",
    "                    if len(digit_history) > 0:\n",
    "                        latest = digit_history.iloc[-1]\n",
    "                        X_test_list.append([latest.get(f, 0) for f in X_train.columns])\n",
    "                    else:\n",
    "                        X_test_list.append([0] * len(X_train.columns))\n",
    "                \n",
    "                X_test = pd.DataFrame(X_test_list, columns=X_train.columns)\n",
    "                \n",
    "                scaler = StandardScaler()\n",
    "                X_train_scaled = scaler.fit_transform(X_train)\n",
    "                X_test_scaled = scaler.transform(X_test)\n",
    "                \n",
    "                model = LogisticRegression(C=1.0, class_weight='balanced', max_iter=1000, random_state=42)\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                \n",
    "                proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "                predicted_top3 = [d for d, _ in sorted(zip(test_data['last_digit'].values, proba), \n",
    "                                                        key=lambda x: x[1], reverse=True)[:3]]\n",
    "                actual_top3 = test_data.nlargest(3, 'current_diff')['last_digit'].tolist()\n",
    "                overlap = len(set(predicted_top3) & set(actual_top3))\n",
    "                precisions.append(overlap / 3)\n",
    "            \n",
    "            if len(precisions) > 0:\n",
    "                set_results.append({\n",
    "                    'event': event_type,\n",
    "                    'precision': np.mean(precisions),\n",
    "                    'n_features': len(X_train.columns)\n",
    "                })\n",
    "        \n",
    "        if len(set_results) > 0:\n",
    "            avg_precision = np.mean([r['precision'] for r in set_results])\n",
    "            avg_n_features = np.mean([r['n_features'] for r in set_results])\n",
    "            \n",
    "            print(f\"\\n平均 Precision@3: {avg_precision:.3f}\")\n",
    "            print(f\"平均特徴量数: {avg_n_features:.0f}\")\n",
    "            \n",
    "            results_summary.append({\n",
    "                'feature_set': feature_set,\n",
    "                'model': model_name,\n",
    "                'avg_precision': avg_precision,\n",
    "                'avg_n_features': avg_n_features\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results_summary)\n",
    "\n",
    "# 実行\n",
    "print(\"拡張特徴量履歴作成中...\")\n",
    "enhanced_histories = {}\n",
    "for name, df in [('all', df_all), ('jug', df_jug), ('non_jug', df_non_jug)]:\n",
    "    if len(df) > 0:\n",
    "        print(f\"\\n【{name.upper()}】\")\n",
    "        enhanced_histories[name] = create_enhanced_history_features(df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"特徴量セット比較実験\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "comparison_results = []\n",
    "for name in ['all', 'jug', 'non_jug']:\n",
    "    if name in enhanced_histories and len(enhanced_histories[name]) > 0:\n",
    "        result = compare_feature_sets(enhanced_histories[name], name.upper(), n_test=5)\n",
    "        comparison_results.append(result)\n",
    "\n",
    "# サマリー表示\n",
    "if len(comparison_results) > 0:\n",
    "    final_summary = pd.concat(comparison_results, ignore_index=True)\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"【総合結果サマリー】\")\n",
    "    print(\"=\"*100)\n",
    "    print(final_summary.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n【ベスト特徴量セット】\")\n",
    "    best = final_summary.loc[final_summary.groupby('model')['avg_precision'].idxmax()]\n",
    "    print(best[['model', 'feature_set', 'avg_precision', 'avg_n_features']].to_string(index=False))\n",
    "\n",
    "print(\"\\n✅ 特徴量エンジニアリング実験完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FLAML AutoML（TOP3ラベル版）\n",
    "# ============================================================\n",
    "\n",
    "# pip install flaml\n",
    "\n",
    "\n",
    "from flaml import AutoML\n",
    "\n",
    "def run_flaml_top3_label(history_df, model_name, event_types=None, n_test=3, time_budget=180):\n",
    "    \"\"\"FLAML TOP3ラベル版（評価指標と訓練目的を一致）\"\"\"\n",
    "    \n",
    "    if event_types is None:\n",
    "        # データ量の多い上位3イベント\n",
    "        event_types = history_df['event_type'].value_counts().head(3).index.tolist()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for event_type in event_types:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"【{event_type.upper()} - {model_name}】\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        event_data = history_df[history_df['event_type'] == event_type].copy()\n",
    "        event_dates = sorted(event_data['date'].unique())\n",
    "        \n",
    "        if len(event_dates) < n_test + 5:\n",
    "            print(f\"データ不足: {len(event_dates)} days\")\n",
    "            continue\n",
    "        \n",
    "        # ウォークフォワード検証\n",
    "        test_dates = event_dates[-n_test:]\n",
    "        \n",
    "        for i, test_date in enumerate(test_dates):\n",
    "            print(f\"\\nTest {i+1}/{n_test}: {test_date}\")\n",
    "            \n",
    "            train_data = event_data[event_data['date'] < test_date].copy()\n",
    "            test_data = event_data[event_data['date'] == test_date].copy()\n",
    "            \n",
    "            # TOP3ラベル作成（訓練データ）\n",
    "            train_data['rank'] = train_data.groupby('date')['current_diff'].rank(ascending=False, method='first')\n",
    "            train_data['label_top3'] = (train_data['rank'] <= 3).astype(int)\n",
    "            \n",
    "            # 特徴量\n",
    "            exclude_cols = ['date', 'last_digit', 'event_type', 'current_diff', 'rank', 'label_top3']\n",
    "            feature_cols = [c for c in train_data.columns if c not in exclude_cols]\n",
    "            \n",
    "            X_train = train_data[feature_cols].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "            y_train = train_data['label_top3'].values\n",
    "            X_test = test_data[feature_cols].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "            \n",
    "            if X_train.shape[1] == 0:\n",
    "                print(\"  特徴量なし\")\n",
    "                continue\n",
    "            \n",
    "            # クラス分布確認\n",
    "            pos_ratio = y_train.mean()\n",
    "            print(f\"  訓練: {len(X_train)} samples × {X_train.shape[1]} features\")\n",
    "            print(f\"  TOP3比率: {pos_ratio:.3f} (期待値: 0.273)\")\n",
    "            \n",
    "            # FLAML実行\n",
    "            automl = AutoML()\n",
    "            \n",
    "            try:\n",
    "                automl.fit(\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    task=\"classification\",\n",
    "                    time_budget=time_budget,\n",
    "                    metric=\"ap\",  # Average Precision（不均衡データ向け）\n",
    "                    estimator_list=['lgbm', 'xgboost', 'rf', 'extra_tree'],\n",
    "                    n_splits=3,\n",
    "                    verbose=0,\n",
    "                    seed=42,\n",
    "                    early_stop=True\n",
    "                )\n",
    "                \n",
    "                # 予測確率\n",
    "                proba = automl.predict_proba(X_test)[:, 1]\n",
    "                \n",
    "                # Precision@3\n",
    "                predicted_top3 = [\n",
    "                    d for d, _ in sorted(\n",
    "                        zip(test_data['last_digit'].values, proba),\n",
    "                        key=lambda x: x[1],\n",
    "                        reverse=True\n",
    "                    )[:3]\n",
    "                ]\n",
    "                actual_top3 = test_data.nlargest(3, 'current_diff')['last_digit'].tolist()\n",
    "                overlap = len(set(predicted_top3) & set(actual_top3))\n",
    "                precision = overlap / 3\n",
    "                \n",
    "                print(f\"  モデル: {automl.best_estimator}\")\n",
    "                print(f\"  Precision@3: {precision:.3f}\")\n",
    "                print(f\"  予測: {predicted_top3} / 実際: {actual_top3}\")\n",
    "                \n",
    "                # 予測確率分布\n",
    "                print(f\"  予測確率範囲: {proba.min():.3f} - {proba.max():.3f}\")\n",
    "                print(f\"  TOP3平均確率: {np.mean([proba[list(test_data['last_digit'].values).index(d)] for d in predicted_top3 if d in test_data['last_digit'].values]):.3f}\")\n",
    "                \n",
    "                # 特徴量重要度（上位10）\n",
    "                try:\n",
    "                    if hasattr(automl.model.estimator, 'feature_importances_'):\n",
    "                        imp = automl.model.estimator.feature_importances_\n",
    "                        top_features = pd.Series(imp, index=feature_cols).nlargest(10)\n",
    "                        print(f\"  重要特徴量TOP10:\")\n",
    "                        for feat, val in top_features.items():\n",
    "                            print(f\"    {feat}: {val:.4f}\")\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                results.append({\n",
    "                    'model_name': model_name,\n",
    "                    'event': event_type,\n",
    "                    'test_date': test_date,\n",
    "                    'precision': precision,\n",
    "                    'best_model': automl.best_estimator,\n",
    "                    'n_features': X_train.shape[1],\n",
    "                    'pos_ratio': pos_ratio,\n",
    "                    'predicted': predicted_top3,\n",
    "                    'actual': actual_top3\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  エラー: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# 実行\n",
    "print(\"=\"*100)\n",
    "print(\"FLAML AutoML（TOP3ラベル版）\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "flaml_top3_results = []\n",
    "for name in ['all', 'jug']:  # 2モデル\n",
    "    if name in enhanced_histories and len(enhanced_histories[name]) > 0:\n",
    "        result = run_flaml_top3_label(\n",
    "            enhanced_histories[name], \n",
    "            name.upper(), \n",
    "            event_types=None,\n",
    "            n_test=3,\n",
    "            time_budget=180\n",
    "        )\n",
    "        if result is not None and len(result) > 0:\n",
    "            flaml_top3_results.append(result)\n",
    "\n",
    "# サマリー\n",
    "if flaml_top3_results:\n",
    "    final_flaml = pd.concat(flaml_top3_results, ignore_index=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"【FLAML TOP3ラベル結果サマリー】\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # モデル別平均\n",
    "    for model_name in final_flaml['model_name'].unique():\n",
    "        subset = final_flaml[final_flaml['model_name'] == model_name]\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  平均Precision@3: {subset['precision'].mean():.3f}\")\n",
    "        print(f\"  最高Precision@3: {subset['precision'].max():.3f}\")\n",
    "        print(f\"  使用モデル: {subset['best_model'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # 詳細結果\n",
    "    print(\"\\n【詳細結果】\")\n",
    "    display_cols = ['model_name', 'event', 'precision', 'best_model']\n",
    "    print(final_flaml[display_cols].to_string(index=False))\n",
    "    \n",
    "    # ベスト結果\n",
    "    best_idx = final_flaml['precision'].idxmax()\n",
    "    best = final_flaml.loc[best_idx]\n",
    "    print(f\"\\n【ベスト結果】\")\n",
    "    print(f\"モデル: {best['model_name']} / イベント: {best['event']}\")\n",
    "    print(f\"Precision@3: {best['precision']:.3f}\")\n",
    "    print(f\"使用モデル: {best['best_model']} ({int(best['n_features'])} features)\")\n",
    "    print(f\"予測: {best['predicted']} / 実際: {best['actual']}\")\n",
    "\n",
    "print(\"\\n✅ FLAML TOP3ラベル版完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ランク予測（回帰→ランク変換）\n",
    "# ============================================================\n",
    "\n",
    "from flaml import AutoML\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def run_rank_regression(history_df, model_name, event_types=None, n_test=3, time_budget=180):\n",
    "    \"\"\"回帰でランク予測 → TOP3 Precision評価\"\"\"\n",
    "    \n",
    "    if event_types is None:\n",
    "        event_types = history_df['event_type'].value_counts().head(3).index.tolist()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for event_type in event_types:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"【{event_type.upper()} - {model_name} (ランク回帰)】\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        event_data = history_df[history_df['event_type'] == event_type].copy()\n",
    "        event_dates = sorted(event_data['date'].unique())\n",
    "        \n",
    "        if len(event_dates) < n_test + 5:\n",
    "            print(f\"データ不足: {len(event_dates)} days\")\n",
    "            continue\n",
    "        \n",
    "        test_dates = event_dates[-n_test:]\n",
    "        \n",
    "        for i, test_date in enumerate(test_dates):\n",
    "            print(f\"\\nTest {i+1}/{n_test}: {test_date}\")\n",
    "            \n",
    "            train_data = event_data[event_data['date'] < test_date].copy()\n",
    "            test_data = event_data[event_data['date'] == test_date].copy()\n",
    "            \n",
    "            # ランクラベル作成（1位=1, 11位=11）\n",
    "            train_data['rank'] = train_data.groupby('date')['current_diff'].rank(\n",
    "                ascending=False, method='first'\n",
    "            ).astype(int)\n",
    "            \n",
    "            # 特徴量\n",
    "            exclude_cols = ['date', 'last_digit', 'event_type', 'current_diff', 'rank']\n",
    "            feature_cols = [c for c in train_data.columns if c not in exclude_cols]\n",
    "            \n",
    "            X_train = train_data[feature_cols].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "            y_train = train_data['rank'].values  # 回帰ターゲット\n",
    "            X_test = test_data[feature_cols].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "            \n",
    "            if X_train.shape[1] == 0:\n",
    "                print(\"  特徴量なし\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  訓練: {len(X_train)} samples × {X_train.shape[1]} features\")\n",
    "            print(f\"  ランク範囲: {y_train.min()}-{y_train.max()}\")\n",
    "            \n",
    "            # FLAML実行（回帰）\n",
    "            automl = AutoML()\n",
    "            \n",
    "            try:\n",
    "                automl.fit(\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    task=\"regression\",  # 回帰問題\n",
    "                    time_budget=time_budget,\n",
    "                    metric=\"mae\",  # Mean Absolute Error\n",
    "                    estimator_list=['lgbm', 'xgboost', 'rf', 'extra_tree'],\n",
    "                    n_splits=3,\n",
    "                    verbose=0,\n",
    "                    seed=42\n",
    "                )\n",
    "                \n",
    "                # 予測ランク\n",
    "                pred_ranks = automl.predict(X_test)\n",
    "                \n",
    "                # 評価指標\n",
    "                mae = mean_absolute_error(\n",
    "                    test_data.groupby('date')['current_diff'].rank(ascending=False, method='first'),\n",
    "                    pred_ranks\n",
    "                )\n",
    "                \n",
    "                # TOP3 Precision計算\n",
    "                # 予測ランクが小さい順（上位）に3つ選ぶ\n",
    "                predicted_top3 = [\n",
    "                    d for d, _ in sorted(\n",
    "                        zip(test_data['last_digit'].values, pred_ranks),\n",
    "                        key=lambda x: x[1]  # ランクが小さい=上位\n",
    "                    )[:3]\n",
    "                ]\n",
    "                actual_top3 = test_data.nlargest(3, 'current_diff')['last_digit'].tolist()\n",
    "                overlap = len(set(predicted_top3) & set(actual_top3))\n",
    "                precision = overlap / 3\n",
    "                \n",
    "                print(f\"  モデル: {automl.best_estimator}\")\n",
    "                print(f\"  MAE（ランク誤差）: {mae:.2f}\")\n",
    "                print(f\"  Precision@3: {precision:.3f}\")\n",
    "                print(f\"  予測: {predicted_top3} / 実際: {actual_top3}\")\n",
    "                \n",
    "                # 予測ランク詳細\n",
    "                rank_df = pd.DataFrame({\n",
    "                    'digit': test_data['last_digit'].values,\n",
    "                    'pred_rank': pred_ranks,\n",
    "                    'actual_diff': test_data['current_diff'].values\n",
    "                }).sort_values('pred_rank')\n",
    "                print(f\"\\n  予測TOP5:\")\n",
    "                for idx, row in rank_df.head(5).iterrows():\n",
    "                    print(f\"    {row['digit']}: 予測ランク{row['pred_rank']:.1f} (実差枚{row['actual_diff']:.0f})\")\n",
    "                \n",
    "                # 特徴量重要度\n",
    "                try:\n",
    "                    if hasattr(automl.model.estimator, 'feature_importances_'):\n",
    "                        imp = automl.model.estimator.feature_importances_\n",
    "                        top_features = pd.Series(imp, index=feature_cols).nlargest(10)\n",
    "                        print(f\"\\n  重要特徴量TOP10:\")\n",
    "                        for feat, val in top_features.items():\n",
    "                            print(f\"    {feat}: {val:.4f}\")\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                results.append({\n",
    "                    'model_name': model_name,\n",
    "                    'event': event_type,\n",
    "                    'test_date': test_date,\n",
    "                    'precision': precision,\n",
    "                    'mae': mae,\n",
    "                    'best_model': automl.best_estimator,\n",
    "                    'n_features': X_train.shape[1],\n",
    "                    'predicted': predicted_top3,\n",
    "                    'actual': actual_top3\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  エラー: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# 実行\n",
    "print(\"=\"*100)\n",
    "print(\"ランク予測（回帰）実験\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "rank_results = []\n",
    "for name in ['all', 'jug']:\n",
    "    if name in enhanced_histories and len(enhanced_histories[name]) > 0:\n",
    "        result = run_rank_regression(\n",
    "            enhanced_histories[name], \n",
    "            name.upper(), \n",
    "            event_types=None,\n",
    "            n_test=3,\n",
    "            time_budget=180\n",
    "        )\n",
    "        if result is not None and len(result) > 0:\n",
    "            rank_results.append(result)\n",
    "\n",
    "# サマリー\n",
    "if rank_results:\n",
    "    final_rank = pd.concat(rank_results, ignore_index=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"【ランク予測結果サマリー】\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # モデル別平均\n",
    "    for model_name in final_rank['model_name'].unique():\n",
    "        subset = final_rank[final_rank['model_name'] == model_name]\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        print(f\"  平均Precision@3: {subset['precision'].mean():.3f}\")\n",
    "        print(f\"  平均MAE: {subset['mae'].mean():.2f} (完璧=0, ランダム≈3.6)\")\n",
    "        print(f\"  最高Precision@3: {subset['precision'].max():.3f}\")\n",
    "        print(f\"  使用モデル: {subset['best_model'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # 詳細結果\n",
    "    print(\"\\n【詳細結果】\")\n",
    "    display_cols = ['model_name', 'event', 'precision', 'mae', 'best_model']\n",
    "    print(final_rank[display_cols].to_string(index=False))\n",
    "    \n",
    "    # ベスト結果\n",
    "    best_idx = final_rank['precision'].idxmax()\n",
    "    best = final_rank.loc[best_idx]\n",
    "    print(f\"\\n【ベスト結果】\")\n",
    "    print(f\"モデル: {best['model_name']} / イベント: {best['event']}\")\n",
    "    print(f\"Precision@3: {best['precision']:.3f}\")\n",
    "    print(f\"MAE: {best['mae']:.2f}\")\n",
    "    print(f\"予測: {best['predicted']} / 実際: {best['actual']}\")\n",
    "\n",
    "print(\"\\n✅ ランク予測完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# セルA: 環境セットアップ + データ準備\n",
    "# ============================================================\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 機械学習\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 設定\n",
    "DB_PATH = 'pachinko_analysis_マルハンメガシティ柏.db'\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# イベント定義\n",
    "EVENT_DEFINITIONS = {\n",
    "    'is_1day': '1day', 'is_2day': '2day', 'is_3day': '3day',\n",
    "    'is_4day': '4day', 'is_5day': '5day', 'is_6day': '6day',\n",
    "    'is_7day': '7day', 'is_8day': '8day', 'is_9day': '9day',\n",
    "    'is_0day': '0day', 'is_39day': '39day', 'is_40day': '40day',\n",
    "    'is_zorome': 'Zorome', 'is_saturday': 'Saturday', 'is_sunday': 'Sunday'\n",
    "}\n",
    "\n",
    "DIGIT_ORDER = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'ゾロ目']\n",
    "\n",
    "print(\"✅ 環境セットアップ完了\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# セルB: データ読み込み\n",
    "# ============================================================\n",
    "\n",
    "def load_last_digit_data(db_path, table_name='last_digit_summary_all'):\n",
    "    \"\"\"last_digit_summaryテーブル読み込み\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    \n",
    "    # イベントカレンダー\n",
    "    df_events = pd.read_sql_query(\"SELECT * FROM event_calendar ORDER BY date\", conn)\n",
    "    \n",
    "    # 末尾データ\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {table_name} ORDER BY date, last_digit\", conn)\n",
    "    df = df.merge(df_events, on='date', how='left')\n",
    "    \n",
    "    # 日付処理\n",
    "    df['date_obj'] = pd.to_datetime(df['date'], format='%Y%m%d')\n",
    "    df['weekday'] = df['date_obj'].dt.day_name()\n",
    "    df['weekday_num'] = df['date_obj'].dt.weekday\n",
    "    df['day_of_month'] = df['date_obj'].dt.day\n",
    "    df['is_saturday'] = (df['weekday_num'] == 5).astype(int)\n",
    "    df['is_sunday'] = (df['weekday_num'] == 6).astype(int)\n",
    "    \n",
    "    # 末尾数値化（カテゴリ型変換前に実施）\n",
    "    df['digit_num'] = df['last_digit'].apply(lambda x: 10 if x == 'ゾロ目' else int(x))\n",
    "    \n",
    "    # カテゴリ型に変換（数値演算後）\n",
    "    df['last_digit'] = pd.Categorical(df['last_digit'], categories=DIGIT_ORDER, ordered=True)\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"データ読込: {len(df)} rows, {df['date'].nunique()} days, {df['last_digit'].nunique()} digits\")\n",
    "    return df\n",
    "\n",
    "# 3モデル分読込\n",
    "df_all = load_last_digit_data(DB_PATH, 'last_digit_summary_all')\n",
    "df_jug = load_last_digit_data(DB_PATH, 'last_digit_summary_jug')\n",
    "df_non_jug = load_last_digit_data(DB_PATH, 'last_digit_summary_other')\n",
    "\n",
    "print(\"\\n✅ データ読み込み完了\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# セルC: イベント履歴ベース特徴量生成（prev_方式）\n",
    "# ============================================================\n",
    "\n",
    "def create_event_history_features(df):\n",
    "    \"\"\"イベント×末尾ごとの履歴特徴量（prev_方式）\"\"\"\n",
    "    all_dates = sorted(df['date'].unique())\n",
    "    \n",
    "    print(\"  イベント×末尾インデックス構築中...\")\n",
    "    event_digit_index = {}\n",
    "    \n",
    "    # イベント×末尾の出現日リストを作成\n",
    "    for date in all_dates:\n",
    "        date_mask = df['date'] == date\n",
    "        date_data = df[date_mask].iloc[0]\n",
    "        \n",
    "        # アクティブイベント特定\n",
    "        active_events = []\n",
    "        for col, label in EVENT_DEFINITIONS.items():\n",
    "            if col in df.columns and date_data.get(col, 0) == 1:\n",
    "                active_events.append(label.lower())\n",
    "        \n",
    "        # 各末尾×イベントの組み合わせに日付を追加\n",
    "        for digit_num in df[date_mask]['digit_num'].unique():\n",
    "            for event_name in active_events:\n",
    "                key = (event_name, digit_num)\n",
    "                if key not in event_digit_index:\n",
    "                    event_digit_index[key] = []\n",
    "                event_digit_index[key].append(date)\n",
    "    \n",
    "    print(f\"  インデックス完成: {len(event_digit_index)} 組み合わせ\")\n",
    "    \n",
    "    # キャッシュ作成\n",
    "    data_cache = {}\n",
    "    for col in ['avg_diff_coins', 'avg_games', 'win_rate', 'last_digit_rank_diff', \n",
    "                'last_digit_rank_games', 'high_profit_rate']:\n",
    "        if col in df.columns:\n",
    "            data_cache[col] = df.set_index(['date', 'digit_num'])[col].to_dict()\n",
    "    \n",
    "    print(\"  prev_特徴量生成中...\")\n",
    "    history_features = []\n",
    "    \n",
    "    for (event_name, digit_num), dates in event_digit_index.items():\n",
    "        for i, date in enumerate(dates):\n",
    "            # 当日データ\n",
    "            current_data = {}\n",
    "            for col, cache in data_cache.items():\n",
    "                current_data[col] = cache.get((date, digit_num))\n",
    "            \n",
    "            if current_data['avg_diff_coins'] is None:\n",
    "                continue\n",
    "            \n",
    "            # prev_1, prev_2, prev_3（同一イベント×末尾の過去）\n",
    "            features = {\n",
    "                'date': date,\n",
    "                'digit_num': digit_num,\n",
    "                'event_type': event_name,\n",
    "                'current_diff': current_data['avg_diff_coins']\n",
    "            }\n",
    "            \n",
    "            # 過去3回分\n",
    "            for j in range(1, 4):\n",
    "                if i >= j:\n",
    "                    prev_date = dates[i - j]\n",
    "                    for col, cache in data_cache.items():\n",
    "                        val = cache.get((prev_date, digit_num))\n",
    "                        features[f'prev_{j}_{col}'] = val if val is not None else 0\n",
    "                else:\n",
    "                    for col in data_cache.keys():\n",
    "                        features[f'prev_{j}_{col}'] = 0\n",
    "            \n",
    "            # 変化量（prev_1との差）\n",
    "            if i >= 1:\n",
    "                for col in ['avg_diff_coins', 'avg_games']:\n",
    "                    if col in data_cache:\n",
    "                        prev_val = data_cache[col].get((dates[i-1], digit_num), 0)\n",
    "                        curr_val = current_data.get(col, 0)\n",
    "                        features[f'prev_1_{col}_change'] = curr_val - prev_val if prev_val else 0\n",
    "            \n",
    "            # 過去N回の統計量\n",
    "            for window in [3, 5]:\n",
    "                if i >= window:\n",
    "                    past_dates = dates[max(0, i-window):i]\n",
    "                    past_diffs = [data_cache['avg_diff_coins'].get((d, digit_num), 0) for d in past_dates]\n",
    "                    past_diffs = [v for v in past_diffs if v is not None]\n",
    "                    \n",
    "                    if len(past_diffs) > 0:\n",
    "                        features[f'prev_{window}_mean_diff'] = np.mean(past_diffs)\n",
    "                        features[f'prev_{window}_max_diff'] = np.max(past_diffs)\n",
    "                        features[f'prev_{window}_std_diff'] = np.std(past_diffs) if len(past_diffs) > 1 else 0\n",
    "                    \n",
    "                    # ランク統計\n",
    "                    if 'last_digit_rank_diff' in data_cache:\n",
    "                        past_ranks = [data_cache['last_digit_rank_diff'].get((d, digit_num), 0) for d in past_dates]\n",
    "                        past_ranks = [v for v in past_ranks if v is not None and v > 0]\n",
    "                        \n",
    "                        if len(past_ranks) > 0:\n",
    "                            features[f'prev_{window}_mean_rank'] = np.mean(past_ranks)\n",
    "                            features[f'prev_{window}_top3_rate'] = sum(1 for r in past_ranks if r <= 3) / len(past_ranks)\n",
    "            \n",
    "            history_features.append(features)\n",
    "    \n",
    "    return pd.DataFrame(history_features)\n",
    "\n",
    "\n",
    "def create_comprehensive_features_v2(df):\n",
    "    \"\"\"イベント履歴ベース + 安全な全日付特徴量\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    print(\"特徴量生成開始（v2: prev_方式）...\")\n",
    "    \n",
    "    # ===== 1. 全日付でのラグ特徴量（直近N日） =====\n",
    "    print(\"  1. 全日付ラグ特徴量...\")\n",
    "    for lag in [1, 7, 14]:\n",
    "        for col in ['avg_diff_coins', 'avg_games', 'win_rate']:\n",
    "            if col in df.columns:\n",
    "                df[f'allday_{col}_lag{lag}'] = df.groupby('digit_num')[col].shift(lag)\n",
    "    \n",
    "    # ===== 2. 曜日×末尾交互作用 =====\n",
    "    print(\"  2. 曜日交互作用...\")\n",
    "    for wd in range(7):\n",
    "        df[f'is_weekday{wd}'] = (df['weekday_num'] == wd).astype(int)\n",
    "        df[f'weekday{wd}_x_digit'] = df[f'is_weekday{wd}'] * df['digit_num']\n",
    "    \n",
    "    # ===== 3. イベント×末尾マッチング =====\n",
    "    print(\"  3. イベントマッチング...\")\n",
    "    event_digit_map = {\n",
    "        '1day': [1], '2day': [2], '3day': [3], '4day': [4], '5day': [5],\n",
    "        '6day': [6], '7day': [7], '8day': [8], '9day': [9], '0day': [0],\n",
    "        '39day': [3, 9], '40day': [4, 0], 'zorome': [10]\n",
    "    }\n",
    "    \n",
    "    for col, label in EVENT_DEFINITIONS.items():\n",
    "        if col in df.columns:\n",
    "            label_lower = label.lower()\n",
    "            if label_lower in event_digit_map:\n",
    "                target_digits = event_digit_map[label_lower]\n",
    "                df[f'match_{label_lower}'] = (\n",
    "                    (df[col] == 1) & (df['digit_num'].isin(target_digits))\n",
    "                ).astype(int)\n",
    "    \n",
    "    # ===== 4. 時系列位置 =====\n",
    "    print(\"  4. 時系列位置...\")\n",
    "    df['days_since_start'] = (df['date_obj'] - df['date_obj'].min()).dt.days\n",
    "    df['days_to_end'] = (df['date_obj'].max() - df['date_obj']).dt.days\n",
    "    \n",
    "    print(f\"  基本特徴量生成完了\")\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"イベント履歴特徴量関数定義完了（prev_方式）\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# セルD: イベント履歴とマージ\n",
    "# ============================================================\n",
    "\n",
    "def merge_event_history_features(df_base, df_history):\n",
    "    \"\"\"基本特徴量とイベント履歴をマージ\"\"\"\n",
    "    print(\"  イベント履歴マージ中...\")\n",
    "    \n",
    "    # date, digit_num, event_typeでマージ\n",
    "    merged_data = []\n",
    "    \n",
    "    for _, row in df_base.iterrows():\n",
    "        date = row['date']\n",
    "        digit_num = row['digit_num']\n",
    "        \n",
    "        # この行のアクティブイベント\n",
    "        active_events = []\n",
    "        for col, label in EVENT_DEFINITIONS.items():\n",
    "            if col in df_base.columns and row.get(col, 0) == 1:\n",
    "                active_events.append(label.lower())\n",
    "        \n",
    "        # 各イベントの履歴特徴量を取得\n",
    "        for event_name in active_events:\n",
    "            history_row = df_history[\n",
    "                (df_history['date'] == date) &\n",
    "                (df_history['digit_num'] == digit_num) &\n",
    "                (df_history['event_type'] == event_name)\n",
    "            ]\n",
    "            \n",
    "            if len(history_row) > 0:\n",
    "                # 基本特徴量 + イベント履歴特徴量\n",
    "                merged_row = row.to_dict()\n",
    "                merged_row['event_type'] = event_name\n",
    "                \n",
    "                # prev_特徴量を追加\n",
    "                for col in history_row.columns:\n",
    "                    if col.startswith('prev_') or col in ['current_diff']:\n",
    "                        merged_row[col] = history_row.iloc[0][col]\n",
    "                \n",
    "                merged_data.append(merged_row)\n",
    "    \n",
    "    result = pd.DataFrame(merged_data)\n",
    "    print(f\"  マージ完了: {len(result)} rows\")\n",
    "    return result\n",
    "\n",
    "\n",
    "    print(\"マージ関数定義完了\")\n",
    "    print(\"  1. ラグ特徴量...\")\n",
    "    for lag in [1, 2, 3, 7, 14, 21, 28]:\n",
    "        for col in ['avg_diff_coins', 'avg_games', 'win_rate', 'last_digit_rank_diff', 'last_digit_rank_games']:\n",
    "            if col in df.columns:\n",
    "                # digit_numでグループ化\n",
    "                df[f'{col}_lag{lag}'] = df.groupby('digit_num')[col].shift(lag)\n",
    "    \n",
    "    # ===== 2. 移動統計量 =====\n",
    "    print(\"  2. 移動統計量...\")\n",
    "    for window in [3, 7, 14, 21, 28]:\n",
    "        for col in ['avg_diff_coins', 'avg_games', 'last_digit_rank_diff']:\n",
    "            if col in df.columns:\n",
    "                # digit_numでグループ化\n",
    "                grouped = df.groupby('digit_num')[col]\n",
    "                df[f'{col}_ma{window}'] = grouped.transform(lambda x: x.rolling(window, min_periods=1).mean())\n",
    "                df[f'{col}_std{window}'] = grouped.transform(lambda x: x.rolling(window, min_periods=1).std())\n",
    "                df[f'{col}_min{window}'] = grouped.transform(lambda x: x.rolling(window, min_periods=1).min())\n",
    "                df[f'{col}_max{window}'] = grouped.transform(lambda x: x.rolling(window, min_periods=1).max())\n",
    "    \n",
    "    # ===== 3. 変化率特徴量 =====\n",
    "    print(\"  3. 変化率...\")\n",
    "    for lag in [1, 7, 14]:\n",
    "        for col in ['avg_diff_coins', 'avg_games']:\n",
    "            if col in df.columns and f'{col}_lag{lag}' in df.columns:\n",
    "                df[f'{col}_change{lag}'] = df[col] - df[f'{col}_lag{lag}']\n",
    "                df[f'{col}_pct_change{lag}'] = df[col] / (df[f'{col}_lag{lag}'].replace(0, 1))\n",
    "    \n",
    "    # ===== 4. ランク関連特徴量 =====\n",
    "    print(\"  4. ランク特徴量...\")\n",
    "    if 'last_digit_rank_diff' in df.columns:\n",
    "        # TOP3率（過去N日間）- digit_numでグループ化\n",
    "        for window in [7, 14, 28]:\n",
    "            df[f'top3_rate_{window}d'] = df.groupby('digit_num')['last_digit_rank_diff'].transform(\n",
    "                lambda x: x.rolling(window, min_periods=1).apply(lambda r: (r <= 3).mean())\n",
    "            )\n",
    "        \n",
    "        # 最高ランク（過去N日間）\n",
    "        for window in [7, 14, 28]:\n",
    "            df[f'best_rank_{window}d'] = df.groupby('digit_num')['last_digit_rank_diff'].transform(\n",
    "                lambda x: x.rolling(window, min_periods=1).min()\n",
    "            )\n",
    "    \n",
    "    # ===== 5. 曜日×末尾 交互作用 =====\n",
    "    print(\"  5. 曜日交互作用...\")\n",
    "    for wd in range(7):\n",
    "        df[f'is_weekday{wd}'] = (df['weekday_num'] == wd).astype(int)\n",
    "        df[f'weekday{wd}_x_digit'] = df[f'is_weekday{wd}'] * df['digit_num']\n",
    "    \n",
    "    # ===== 6. イベント×末尾 マッチング =====\n",
    "    print(\"  6. イベントマッチング...\")\n",
    "    event_digit_map = {\n",
    "        '1day': [1], '2day': [2], '3day': [3], '4day': [4], '5day': [5],\n",
    "        '6day': [6], '7day': [7], '8day': [8], '9day': [9], '0day': [0],\n",
    "        '39day': [3, 9], '40day': [4, 0], 'zorome': [10]\n",
    "    }\n",
    "    \n",
    "    for col, label in EVENT_DEFINITIONS.items():\n",
    "        if col in df.columns:\n",
    "            label_lower = label.lower()\n",
    "            if label_lower in event_digit_map:\n",
    "                target_digits = event_digit_map[label_lower]\n",
    "                # digit_numを使用（数値型）\n",
    "                df[f'match_{label_lower}'] = (\n",
    "                    (df[col] == 1) & (df['digit_num'].isin(target_digits))\n",
    "                ).astype(int)\n",
    "    \n",
    "    # ===== 7. イベント履歴特徴量 =====\n",
    "    print(\"  7. イベント履歴...\")\n",
    "    # 各イベントでの過去実績\n",
    "    for col, label in EVENT_DEFINITIONS.items():\n",
    "        if col in df.columns:\n",
    "            event_mask = df[col] == 1\n",
    "            event_data = df[event_mask].copy()\n",
    "            \n",
    "            if len(event_data) > 0:\n",
    "                # last_digitを文字列に変換してgroupby\n",
    "                event_data['digit_str'] = event_data['digit_num'].astype(str)\n",
    "                \n",
    "                # イベント時の平均差枚（末尾別）- expanding mean\n",
    "                event_avg_dict = {}\n",
    "                for digit_str in event_data['digit_str'].unique():\n",
    "                    digit_data = event_data[event_data['digit_str'] == digit_str]['avg_diff_coins']\n",
    "                    expanding_mean = digit_data.expanding(min_periods=1).mean()\n",
    "                    event_avg_dict[digit_str] = expanding_mean\n",
    "                \n",
    "                # DataFrameに格納\n",
    "                df[f'{label.lower()}_hist_avg'] = 0.0\n",
    "                for digit_str, expanding_mean in event_avg_dict.items():\n",
    "                    mask = event_mask & (df['digit_num'] == int(digit_str))\n",
    "                    df.loc[mask, f'{label.lower()}_hist_avg'] = expanding_mean.values\n",
    "                \n",
    "                # 前方埋め（次回イベント予測用）- digit_numでグループ化\n",
    "                df[f'{label.lower()}_hist_avg'] = df.groupby('digit_num')[f'{label.lower()}_hist_avg'].ffill()\n",
    "    \n",
    "    # ===== 8. 組み合わせ特徴量 =====\n",
    "    print(\"  8. 組み合わせ...\")\n",
    "    if 'avg_diff_coins' in df.columns and 'avg_games' in df.columns:\n",
    "        df['efficiency'] = df['avg_diff_coins'] / df['avg_games'].replace(0, 1)\n",
    "        # digit_numでグループ化\n",
    "        df['efficiency_lag1'] = df.groupby('digit_num')['efficiency'].shift(1)\n",
    "        df['efficiency_ma7'] = df.groupby('digit_num')['efficiency'].transform(\n",
    "            lambda x: x.rolling(7, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    # ===== 9. 時系列位置特徴量 =====\n",
    "    print(\"  9. 時系列位置...\")\n",
    "    df['days_since_start'] = (df['date_obj'] - df['date_obj'].min()).dt.days\n",
    "    df['days_to_end'] = (df['date_obj'].max() - df['date_obj']).dt.days\n",
    "    \n",
    "    # ===== 10. 統計的特徴量 =====\n",
    "    print(\"  10. 統計量...\")\n",
    "    for window in [7, 14]:\n",
    "        if 'avg_diff_coins' in df.columns:\n",
    "            # digit_numでグループ化\n",
    "            grouped = df.groupby('digit_num')['avg_diff_coins']\n",
    "            \n",
    "            # 歪度\n",
    "            skew_col = f'diff_skew{window}'\n",
    "            df[skew_col] = grouped.transform(\n",
    "                lambda x: x.rolling(window, min_periods=3).skew()\n",
    "            )\n",
    "            \n",
    "            # 尖度\n",
    "            kurt_col = f'diff_kurt{window}'\n",
    "            df[kurt_col] = grouped.transform(\n",
    "                lambda x: x.rolling(window, min_periods=3).apply(\n",
    "                    lambda y: y.kurtosis() if len(y) > 2 else 0\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    print(f\"  生成完了: {len([c for c in df.columns if c not in ['date', 'last_digit', 'date_obj', 'weekday']])} 特徴量\")\n",
    "    return df\n",
    "\n",
    "print(\"網羅的特徴量生成関数定義完了\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# セルE: イベント別データ分割 + 重要度評価\n",
    "# ============================================================\n",
    "\n",
    "def prepare_event_data(df_merged, event_name, n_test=3):\n",
    "    \"\"\"イベント別データ準備（prev_特徴量版）\"\"\"\n",
    "    event_data = df_merged[df_merged['event_type'] == event_name].copy()\n",
    "    event_dates = sorted(event_data['date'].unique())\n",
    "    \n",
    "    if len(event_dates) < n_test + 5:\n",
    "        return None, None, None, None, None, None\n",
    "    \n",
    "    # 訓練：最後のn_test日を除く全て\n",
    "    # テスト：最後のn_test日\n",
    "    train_dates = event_dates[:-n_test]\n",
    "    test_dates = event_dates[-n_test:]\n",
    "    \n",
    "    train_data = event_data[event_data['date'].isin(train_dates)].copy()\n",
    "    test_data = event_data[event_data['date'].isin(test_dates)].copy()\n",
    "    \n",
    "    # TOP3ラベル作成\n",
    "    train_data['rank'] = train_data.groupby('date')['current_diff'].rank(ascending=False, method='first')\n",
    "    train_data['is_top3'] = (train_data['rank'] <= 3).astype(int)\n",
    "    \n",
    "    test_data['rank'] = test_data.groupby('date')['current_diff'].rank(ascending=False, method='first')\n",
    "    test_data['is_top3'] = (test_data['rank'] <= 3).astype(int)\n",
    "    \n",
    "    # 🔴 prev_特徴量のみ使用（完全リーク防止）\n",
    "    safe_patterns = [\n",
    "        'prev_',  # イベント履歴特徴量\n",
    "        'allday_',  # 全日付ラグ\n",
    "        'weekday', 'day_of_month', 'is_saturday', 'is_sunday',\n",
    "        'days_since_start', 'days_to_end',\n",
    "        'is_1day', 'is_2day', 'is_3day', 'is_4day', 'is_5day',\n",
    "        'is_6day', 'is_7day', 'is_8day', 'is_9day', 'is_0day',\n",
    "        'is_39day', 'is_40day', 'is_zorome',\n",
    "        'match_', 'digit_num'\n",
    "    ]\n",
    "    \n",
    "    # メタ情報除外\n",
    "    exclude_cols = ['date', 'date_obj', 'last_digit', 'weekday', 'event_type',\n",
    "                   'current_diff', 'rank', 'is_top3']\n",
    "    \n",
    "    feature_cols = []\n",
    "    for c in train_data.columns:\n",
    "        if c in exclude_cols:\n",
    "            continue\n",
    "        if train_data[c].dtype not in ['int64', 'float64']:\n",
    "            continue\n",
    "        if any(pattern in c for pattern in safe_patterns):\n",
    "            feature_cols.append(c)\n",
    "    \n",
    "    print(f\"  使用特徴量: {len(feature_cols)} 個\")\n",
    "    \n",
    "    # prev_特徴量の内訳表示\n",
    "    prev_features = [c for c in feature_cols if c.startswith('prev_')]\n",
    "    other_features = [c for c in feature_cols if not c.startswith('prev_')]\n",
    "    print(f\"    - prev_特徴量: {len(prev_features)}\")\n",
    "    print(f\"    - その他: {len(other_features)}\")\n",
    "    \n",
    "    X_train = train_data[feature_cols].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    y_train = train_data['is_top3'].values\n",
    "    X_test = test_data[feature_cols].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    y_test = test_data['is_top3'].values\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, test_data, feature_cols, leak_keywords\n",
    "        \n",
    "    # 安全パターンに該当するか\n",
    "    is_safe = any(pattern in c for pattern in safe_patterns)\n",
    "        \n",
    "    if not is_leak and is_safe:\n",
    "        feature_cols.append(c)\n",
    "    else:\n",
    "        excluded_features.append(c)\n",
    "    \n",
    "    print(f\"  全カラム: {len(train_data.columns)}\")\n",
    "    print(f\"  使用特徴量: {len(feature_cols)} 個\")\n",
    "    print(f\"  除外: {len(excluded_features)} 個\")\n",
    "    \n",
    "    # 除外された重要そうな特徴量を表示\n",
    "    if excluded_features:\n",
    "        print(f\"  除外例: {excluded_features[:15]}\")\n",
    "    \n",
    "    X_train = train_data[feature_cols].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    y_train = train_data['is_top3'].values\n",
    "    X_test = test_data[feature_cols].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    y_test = test_data['is_top3'].values\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, test_data, feature_cols\n",
    "\n",
    "\n",
    "def evaluate_feature_importance(df_merged, event_name):\n",
    "    \"\"\"特徴量重要度評価（3モデル比較）- prev_方式\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"【{event_name.upper()}】重要度評価\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # データ準備\n",
    "    result = prepare_event_data(df_merged, event_name, n_test=3)\n",
    "    if result[0] is None:\n",
    "        print(f\"データ不足: {event_name}\")\n",
    "        return None\n",
    "    \n",
    "    X_train, y_train, X_test, y_test, test_data, feature_cols = result\n",
    "    \n",
    "    print(f\"訓練: {len(X_train)} samples × {len(feature_cols)} features\")\n",
    "    print(f\"テスト: {len(X_test)} samples\")\n",
    "    print(f\"TOP3比率: train={y_train.mean():.3f}, test={y_test.mean():.3f}\")\n",
    "    \n",
    "    # モデル訓練\n",
    "    models = {\n",
    "        'RF': RandomForestClassifier(\n",
    "            n_estimators=200, max_depth=10, min_samples_split=5,\n",
    "            random_state=42, n_jobs=-1, class_weight='balanced'\n",
    "        ),\n",
    "        'XGB': XGBClassifier(\n",
    "            n_estimators=200, max_depth=6, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            random_state=42, eval_metric='logloss'\n",
    "        ),\n",
    "        'LGBM': LGBMClassifier(\n",
    "            n_estimators=200, max_depth=6, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            random_state=42, verbose=-1, class_weight='balanced'\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- {name} ---\")\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # 予測\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            proba = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            proba = model.predict(X_test)\n",
    "        \n",
    "        # Precision@3計算（日別）\n",
    "        precisions = []\n",
    "        for date in test_data['date'].unique():\n",
    "            date_mask = test_data['date'] == date\n",
    "            date_proba = proba[date_mask]\n",
    "            date_digits = test_data[date_mask]['last_digit'].values\n",
    "            date_actual = test_data[date_mask]['current_diff'].values\n",
    "            \n",
    "            # 予測TOP3\n",
    "            pred_top3 = [d for d, _ in sorted(zip(date_digits, date_proba), \n",
    "                                              key=lambda x: x[1], reverse=True)[:3]]\n",
    "            # 実際TOP3\n",
    "            actual_top3 = [d for d, _ in sorted(zip(date_digits, date_actual),\n",
    "                                                key=lambda x: x[1], reverse=True)[:3]]\n",
    "            \n",
    "            overlap = len(set(pred_top3) & set(actual_top3))\n",
    "            precisions.append(overlap / 3)\n",
    "        \n",
    "        avg_precision = np.mean(precisions)\n",
    "        print(f\"Precision@3: {avg_precision:.3f}\")\n",
    "        \n",
    "        # 特徴量重要度\n",
    "        importances = model.feature_importances_\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'precision': avg_precision,\n",
    "            'importance': importance_df,\n",
    "            'test_proba': proba,\n",
    "            'test_data': test_data\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# セルF: 全イベント一括評価（prev_方式）\n",
    "# ============================================================\n",
    "\n",
    "def run_comprehensive_evaluation(df, model_name='ALL'):\n",
    "    \"\"\"全イベントで重要度評価実行（prev_方式）\"\"\"\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"【{model_name}】包括的評価開始（prev_方式）\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    # 1. 基本特徴量生成\n",
    "    print(\"\\n基本特徴量生成中...\")\n",
    "    df_base = create_comprehensive_features_v2(df)\n",
    "    \n",
    "    # 2. イベント履歴特徴量生成\n",
    "    print(\"\\nイベント履歴特徴量生成中...\")\n",
    "    df_history = create_event_history_features(df)\n",
    "    \n",
    "    print(f\"\\nイベント履歴データ: {len(df_history)} rows\")\n",
    "    print(f\"ユニークイベント: {df_history['event_type'].nunique()}\")\n",
    "    \n",
    "    # 3. マージ\n",
    "    print(\"\\n特徴量マージ中...\")\n",
    "    df_merged = merge_event_history_features(df_base, df_history)\n",
    "    \n",
    "    print(f\"マージ後: {len(df_merged)} rows\")\n",
    "    print(f\"特徴量数: {len(df_merged.columns)}\")\n",
    "    \n",
    "    # 4. 評価対象イベント（データ量上位5個）\n",
    "    event_counts = df_merged['event_type'].value_counts().head(5)\n",
    "    \n",
    "    print(f\"\\n評価対象: {len(event_counts)} イベント\")\n",
    "    for event_name, count in event_counts.items():\n",
    "        print(f\"  {event_name}: {count} rows\")\n",
    "    \n",
    "    # 5. 各イベントで評価\n",
    "    all_results = {}\n",
    "    \n",
    "    for event_name in event_counts.index:\n",
    "        results = evaluate_feature_importance(df_merged, event_name)\n",
    "        if results:\n",
    "            all_results[event_name] = results\n",
    "    \n",
    "    return all_results, df_merged\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# セルG: 重要度可視化\n",
    "# ============================================================\n",
    "\n",
    "def visualize_importance(all_results, top_n=30):\n",
    "    \"\"\"特徴量重要度可視化\"\"\"\n",
    "    for event_name, results in all_results.items():\n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(f\"【{event_name}】重要特徴量TOP{top_n}\")\n",
    "        print(f\"{'='*100}\\n\")\n",
    "        \n",
    "        # 3モデル比較\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "        \n",
    "        for idx, (model_name, result) in enumerate(results.items()):\n",
    "            importance_df = result['importance'].head(top_n)\n",
    "            \n",
    "            ax = axes[idx]\n",
    "            ax.barh(range(len(importance_df)), importance_df['importance'].values)\n",
    "            ax.set_yticks(range(len(importance_df)))\n",
    "            ax.set_yticklabels(importance_df['feature'].values, fontsize=8)\n",
    "            ax.invert_yaxis()\n",
    "            ax.set_xlabel('Importance', fontsize=10)\n",
    "            ax.set_title(f'{model_name} (Precision@3: {result[\"precision\"]:.3f})', \n",
    "                        fontsize=12, fontweight='bold')\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # テキストレポート\n",
    "        print(f\"\\n【モデル別Precision@3】\")\n",
    "        for model_name, result in results.items():\n",
    "            print(f\"{model_name}: {result['precision']:.3f}\")\n",
    "        \n",
    "        print(f\"\\n【共通重要特徴量TOP15】\")\n",
    "        # 3モデルの順位を平均\n",
    "        all_features = set()\n",
    "        for result in results.values():\n",
    "            all_features.update(result['importance']['feature'].head(30).tolist())\n",
    "        \n",
    "        feature_ranks = {f: [] for f in all_features}\n",
    "        for result in results.values():\n",
    "            imp_dict = result['importance'].set_index('feature')['importance'].to_dict()\n",
    "            for f in all_features:\n",
    "                feature_ranks[f].append(imp_dict.get(f, 0))\n",
    "        \n",
    "        avg_importance = {f: np.mean(ranks) for f, ranks in feature_ranks.items()}\n",
    "        sorted_features = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for rank, (feat, imp) in enumerate(sorted_features[:15], 1):\n",
    "            print(f\"{rank:2d}. {feat:50s} {imp:.6f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# セルH: 実行\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"prev_方式 特徴量重要度評価 開始\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# 実行（ALLモデルのみ、時間短縮のため）\n",
    "all_results, df_merged = run_comprehensive_evaluation(df_all, 'ALL')\n",
    "\n",
    "# 重要度可視化\n",
    "visualize_importance(all_results, top_n=30)\n",
    "\n",
    "print(\"\\n✅ prev_方式 特徴量重要度評価完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
